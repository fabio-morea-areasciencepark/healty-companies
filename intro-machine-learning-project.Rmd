--- 
title: 'Machine Learning project: predicting financial rating of companies'
author: "Fabio Morea"
date: "`r Sys.Date()`"
output:
  pdf_document:
    # toc: yes
    # toc_depth: '1'
    fig_width: 6
    fig_height: 3

---

> **Abstract**: This notebook describes the final project for the course *introduction to machine learning* and, at the same time, a case study for Area Science Park in the frame of Innovation Intelligence FVG project.  A binary classifier has been used to predict whether a company has a "high" financial rating, The assessment is performed on a dataset of companies in manufacturing sector in Friuli Venezia Giulia, using two definition of the label, with a balanced or imbalanced dataset respectively. The experimental assessment of predictions is measured by accuracy, sensitivity and specificity.



```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
#libraries
require(tidyverse)
require(ggthemes)
require(patchwork)
require(ggcorrplot)  
require(scales)

require(rpart)
require(rpart.plot)

require(caret)
require(DMwR2) # for smote implementation
require(pROC)  # for AUC (area under the curve) 

#default values
theme_set( theme_hc(base_size = 12))
set.seed(123)
```

```{r functions, include=FALSE}
# captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
``` 
 

# 1. Introduction and problem statement

## Background information

Research, innovation and highly skilled people are considered to be important factors in economic and social development. Economic support policies often include funds to support research (for example with the creation of public research infrastructures), companies (for example with tenders to co-finance innovative projects) and the training of people with the necessary skills.

Area Science Park [1] is a national research institution that manages a science and technology park located in Trieste (Italy) and is engaged in several projects aiming to support innovation at the regional level. Innovation Intelligence FVG [2] is a project, managed by Area Science Park and supported by Regione Friuli Venezia Giulia, which aims to monitor the performance of companies in terms of economic, employment and innovation results.  For more info refer to: (www.innovationintelligence.it).

In the frame of Innovation Inteligence project, Area Science Park needs to analyse the performance of groups of companies (such as the *tenants of Area Science Park*, about 60 companies that have their premises or research laboratories in the science and technology park, or the *regional cluster of metal and plastic manufacturing*, identified by a list of NACE activity sectors [3]) comparing them against similar groups in other regions. Information on companies is available in several datasets that either open source or available under a license for the project partners. A relevant source of information, **financial rating** is available from a rating agency [4], only for the companies that have their premises in Friuli Venezia Giulia. 

Financial rating is a numerical variable ranging from 1 to 10, where low values denote an insufficient capability to meet financial obligations, and high values denote very good or excellent reliability. The value is generated by a prorpietary algorithm summarizing the overall performance of a company in all its economic and financial areas: profitability, liquidity, solvency, efficiency, production. 
Only a part of the data used to generate financial ratings is available for this project, thus we may expect that the actual value will be hard to predict. According to some similar cases described in literature [7, 8] reasonably good predictions of rating can be based on public financial indicators (balance sheet data). In our case we can and information on employees (number of employees, turnover and net balance in a given year), that are a good proxy for company health and performance.

## Objective and constraints

The Innovation Intelligence team at Area Science Park wants to explore the potential use of Machine Learning techniques to improve to predict the financial rating of companies, based on other relevant datasets (general company information from the *Italian Business Register* [5].

> The **objective** of the project is to assess the quality of predictions of a machine learnig solutions capable of predicting whether a company belongs to the "top" gropup, defined on the basis of financial rating. 

The analysis should be applied to a given case study: companies working in the sector of metal and plastic manufacturing in Friuli Venezia Giulia (nace sectors 23 to 29). 
The **target audience** of the feasibility study is the Innovation Intelligence team at Area Science Park, a small group of economic analysts, that have a robust domain knowledge but limited experience in data science.

The **expected result** is a short report (PDF file) and the supporting R-project (a folder including .Rmd and .Rproj files) presenting a detailed case study, including a short introduction to model selection, feature engineering and experimental assessment of model performance. The result should include the following cases: a *balanced problem*, using a solution that is explainable and interpretable and an *imbalanced case* using a more advanced solution without constraints on explainability and interpretability.

**Constraints: ** training and classification are generally performed on a laptop, once a month, therefore there are no specific constraints on time or computation effort. The number of companies involved in each target group ranges generally between 200 and 2000. 

## Formal statement of the problem

Let a company be represented by a vector *X* in a multidimensional space, and associated with a binary label *y* representing whether the company belongs to a group of "top performers" or not. The objective is to assess the performance of a binary classification model that predicts *y* under the following conditions: 

- dataset composed of at least 1000 observations, homogeneous by sector and and company type
- computation time < 1 hour on a state-of-the art personal computer
- case B (balanced, minority class < 40%)
- case I (inbalanced, minority class < 20%)

# 2. Assessment and performance indexes

The performance of different methods will be assessed experimentally, by a k-fold cross validation procedure.
Three indexes will be used to measure performance: accuracy, sensitivity and specificty.

$$Accuracy = (True Positive +True Negative)/AllCases$$
$$Sensitivity = True Positive / (True Positive + False Negative)$$ 
$$Specificity = True Negative/ (True Negative + False Positive)$$

Assessment of binary decision trees will be performed on a customized function *assessAccuracySensitivitySpecificity()*, while the random forrest will be assessed using *caret* package. Further information on the procedures can be found in the Annexes.

As the objective is to meausre the performance, there is no predetermined threshold for the indexes. However, according to literature and previous experience, accuracy, sensitivity and selectivity may be expected to be of the order of 80%. Such threshold may seem relatively high compared if compared to safety-critical applications, but is sufficient for the business case and in line with similar cases of binary classification of company performance found in scientific literature. 

# 3. Proposed solution

The proposed solution is to evaluate the performance using two applied to two case studies: *Top7* (balanced) and *Top8*(unbalanced). Each model will be assessed using (a)  *binary decision tree* (that offers the advantage of explainability and interpretability) and (b) *random forrest*. that offers potentially a higher performance. 

1) **pre-processing**: select and rescale features to be processed (matrix X) and generate the labels to be predicted y7 for Top7 and y8 for Top8.
```{r, echo= FALSE}
cp_fixed = .001
```

2) test both datasets uning a binary decision tree, optimizing the tree parameters to achieve maximum accuracy (using nested k-fold cross validation). The tree will be tuned using the parameter minsplit, and setting the parameter cp to a fixed value (cp = .001) in order to allow a complexity budget, and keep the model reasonably simple to enhance explainability and interpretability. 
3) test both datasets uning random forrest
4) compare final results


# 4. Experimental evaluation

## 4.1 Data

The data available from Innovation Intelligence needs to be pre-processed in order to obtain a *tidy* dataset suitable for ML. An extended explanation of all the features available in the original dataset is available in the Annexes.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
require(tidyverse)
pathTidyData = './../../_data/tidy/'
X <- read_csv(paste0(pathTidyData,"X.csv")) %>% mutate(idCompany = as.factor(idCompany))
ys <- read_csv(paste0(pathTidyData,"y.csv")) %>% mutate(idCompany = as.factor(idCompany))
```

At this stage the dataset X consists of n = `r nrow(data)` observations and p = `r ncol(data)` features (namely: `r names(data)` ). 
The sample is selected according to **NACE codes** and **company type**. The first selection on company type : we select all types that have a duty of disclosure of financial information, and therefore are suitable for the analysis, namely SU (società a responsabilità limitata con unico socio), SR (società a responsabilità limitata), SP (società per azioni), SD (società europea),  RS (società a responsabilità limitata semplificata), RR (società a responsabilità limitata a capitale ridotto), AU (società  per azioni con socio unico), AA (società in accomandita per azioni.

A further selection is based on **NACE codes** (further information in the Annexes).The selected sample is composed of companies that have at least one NACE code in one of the following Divisions: 22 (Manufacture of rubber and plastic products), 23 (Manufacture of other non-metallic mineral products), 24 (Manufacture of basic metals), 25 (Manufacture of fabricated metal products, except machinery and equipment), 26 (Manufacture of computer, electronic and optical products), 27 (Manufacture of electrical equipment) and 28 (Manufacture of machinery and equipment).

Some filters are applied: time-dependent data (such as balaance sheet data, rating and employees flows) are filtered to year 2019, and company age (years in business) is filtered to anly value greater than 1.

Labels y7 and y8 are generated on the basisi of financial ratings. Data is  available for years 2018, 2019 and 2020; for the purpose of this study a single year (2019) will be selected.   The choice of classes highligts a relavant issue in classification: imbalance in the distirbution of labels.The balanced dataset isTop7 groups basically all above-average companies, while isTop8 groups about one quarter of the companies.  In the folloqing, the underrepresented class will be referredd to as *minority class*, and the over represented class is referred to as *majority class*.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
proportions <- ys %>% pivot_longer(cols=c( isTop8, isTop7)) %>%
  group_by(name) %>%
  summarise(minority.class = round(mean(value),3)) %>%
  mutate(majority.class = 1 - minority.class)
knitr::kable(proportions)

```


The objective of feature engineering is to build a dataset that contains a set of relevant variables for learning and prediction, appropriately scaled, in the form of a matrix. Specifically we will focus on calculating new features based on domain knowledge, checking variable correlation and normalizing the selected features by centering and scaleing. 
The original features are higly correlated, as higlighted in the following correlation matrix.  

Moreover, featur values range over different orders of magnitude (company age ranges from 1 to 150, while total assets ranges from 0 to $10^9$ €).

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
X <- X %>% select(idCompany, totAssets,totEquity, noi,personnel,prod, debts,deprec,valCost,totIntang, revenues,valAdded,yearsInBusiness, staffTurnover, staffBalance, StockAll) %>% na.omit()
#correlation matrix
cm <- X %>% select(where(is.numeric), -idCompany) %>% cor() 
p.fe.1 <-ggcorrplot(cm,  type = "upper",insig = "blank") +
  theme(axis.text.x=element_text(size=6, angle=90))+
  theme(axis.text.y=element_text(size=6, angle=0))+
  labs(x="", y="") 
  
#boxplot
p.fe.2 <- ggplot(stack(X), aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="") +
 geom_boxplot(fill = "white", colour = "black") + coord_flip() +
  theme(axis.text.x=element_text(size=12, angle=0))+
  theme(axis.text.y=element_text(size=6, angle=0))+
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
              labels = trans_format("log10", math_format(10^.x)))+ 
  annotation_logticks(sides='b')+
  theme(axis.text.x = element_text(vjust = -2))+ theme_excel_new() 
```


```{r, fig.cap=paste("Correlation and order of magnitude of original features"), error=FALSE, warning=FALSE, message=FALSE}
wrap_plots(p.fe.1, p.fe.2)
```


We can tackle both issues by calculating new features that scale economic values to the company size, a common practice in economi analusys, that allows direct comparison of company performance regardless of comapny size. the rev features are named 'rel*' as they are scaled to the total assets (totAssets) or the total number of employees (StockAll) of each company.


```{r echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
X <- X %>% 
    mutate(relInt =   totIntang/totAssets*5) %>%
    mutate(relEquity = totEquity/totAssets) %>%
    mutate(relreve   = revenues/totAssets) %>%
    mutate(relNoi    = noi/totAssets) %>%
    mutate(relPers   = personnel/totAssets) %>%
    mutate(relDeprec = deprec/totAssets*5) %>%
    mutate(yearsInBusiness = yearsInBusiness)  %>%
    mutate(relStaffTurnover = staffTurnover/StockAll) %>%
    mutate(relStaffBalance  = staffBalance/StockAll) %>%
    mutate(VAS  = valAdded/StockAll) %>%
    mutate(logStaff  = log10(StockAll)) %>%
    select(-totAssets,-totEquity, -noi,-personnel,-debts,-prod,
          -deprec,-revenues,-valCost,-totIntang,-valAdded,
          -StockAll, -staffTurnover, -staffBalance)%>% na.omit()
```
Correlation between the new features has significantly improved, as shown in the correlation matrix below.  

```{r echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
cm1 <- X %>% select(where(is.numeric)) %>% cor() 
p.fe.3 <-ggcorrplot(cm1,   type = "upper", insig = "blank") +
  theme(axis.text.x=element_text(size=6, angle=90))+
  theme(axis.text.y=element_text(size=6, angle=0))
```

The next step is to normalize (center and rescale) numeric features to the a similar range in order to improve the performance of the learning algorithm. 

```{r echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
X <- X %>% mutate(across(is.numeric, ~ as.numeric(scale(.)))) 
p.fe.4 <- X %>%  select(-idCompany)  %>% stack() %>% ggplot(aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="normalized features") +
 geom_boxplot(fill = "light gray", colour = "black") + coord_flip()  +  scale_y_continuous(  limit = c(-3, 3)) + geom_hline(yintercept=0, color = "red") +
  theme(axis.text.x=element_text(size=6, angle=90))+
  theme(axis.text.y=element_text(size=6, angle=0))

```

```{r, fig.cap=paste("Correlation and distribution of after features engineering")}
wrap_plots(p.fe.3,p.fe.4)
```

All features are of the same order of magnitude. The dataset is composed of n = r nrow(data) observations and p = r ncol(data) features (namely: `r names(data)` ). There meaning of variables is  self explanatory in some cases, but economic features may require an explanation:  


```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
# this procedure is part of the Annexes
data <- X %>% inner_join(ys) %>% select(-idCompany)

y7 <- ys %>% select(idCompany,isTop7)  %>% 
  mutate(isTop = as.factor(isTop7)) %>% 
  select(-isTop7)%>%
  mutate(idCompany = as.factor(idCompany)) 
data7 <- X %>% inner_join(y7) %>% select(-idCompany)


y8 <- ys %>% select(idCompany,isTop8)  %>% 
  mutate(isTop = as.factor(isTop8)) %>% 
  select(-isTop8)%>%
  mutate(idCompany = as.factor(idCompany))
data8 <- X %>% inner_join(y8) %>% select(-idCompany)

p.learn = .8
indexes.learning = sample(c(1:nrow(data)))[1:(nrow(data)*p.learn)]
data7.learn = data7[ indexes.learning,]
data7.test =  data7[-indexes.learning,]
data8.learn = data8[ indexes.learning,]
data8.test =  data8[-indexes.learning,]
```

## 4.2 Procedure

### Procedure for auto tuning of trees

The quality of prediction of a single decision tree can be assessed using different parameters based on confusion matrixsuch: error rate, accuracy, specificity and sensitivity. A **confusion matrix** is a table (2x2 in the case of binary classifiers) that shows the number of values classified correctly (on the main diagonal) or misclassified (off-diagonal). 

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
# this procedure is part of the Annexes
computeConfusionMatrix = function(model, data) {
  predicted.y = predict(model, data, type="class")
  return(table(predicted.y, data$isTop))
}
```

Learning and prediction are based a stocastic process (splitting data into learn and test sets), hence the results vary for each sample. In order to evaluate te average performance of the method, we need to assess it over a number of samples using a structured experimental procedure such as k-fold cross validation. 
 
```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
# this procedure is part of the Annexes
computeAccuracySensitivitySpecificity =  function(model, data) {
  conf.matrix <- computeConfusionMatrix (model, data)
  true.neg  <- conf.matrix[1,1]
  false.neg <- conf.matrix[1,2]
  false.pos <- conf.matrix[2,1]
  true.pos  <- conf.matrix[2,2]
  pos = true.pos + false.neg
  neg = true.neg + false.pos
  accuracy <- (true.pos + true.neg) / (pos+neg)
  sensitivity <- true.pos/(true.pos+false.neg) # Sensitivity: The “true positive rate” 
  specificity <- true.neg/(true.neg+false.pos)# Specificity: The “true negative rate”
  return( c(accuracy, sensitivity, specificity) )
}
```

Accuracy of the the regression can be assessed as in the following example: the tree is trained on data.learn, then performance is assessed on data.test. 


The learning and test datasets are definied as a random sample of the data, therefore they are different every time the program is executed. The model (and its performance) vary accordingly. In order to achieve a stable performance assessment we need to repeat the procedure several times, and return the average result.


The learning and test datasets are definied as a random sample of the data, therefore they are different every time the program is executed. The model (and its performance) vary accordingly. In order to achieve a stable performance assessment we need to repeat the procedure several times, and return the average result.


```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
compute.Kfold.AccuracySensitivitySpecificity <- function(data,  k_folds=10, minsplit, cp) {
  data_f <- data %>% sample_frac(1) %>% 
  group_by(isTop) %>% 
  mutate(fold=rep(1:k_folds, length.out=n())) %>% ungroup
  
  folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  for(i in 1:k_folds){
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    
    # learn on  data.other.folds
    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = minsplit, cp = cp)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )

    #save results
    folds.results <-  folds.results %>% 
    add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = minsplit)
  }
  
  return(folds.results)
}
```

The procedure for optimizing the tree is in 2 steps: first scan a large range of the complexity parameter to identify the minsplit candidates, second optimize the tree on a specific candidate. This procedure is performed using a nested k-fold cross validation.

### Procedure for random Forrest
....

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
# specific for RF
compute.Kfold.AccuracySensitivitySpecificity.RF <- function(data,  model, k_folds=10) {
  data_f <- data %>% sample_frac(1) %>% 
  group_by(isTop) %>% 
  mutate(fold=rep(1:k_folds, length.out=n())) %>% ungroup
  
  folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  for(i in 1:k_folds){
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    
    # learn on  data.other.folds
   
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )

    #save results
    folds.results <-  folds.results %>% 
    add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = minsplit)
  }
  
  return(folds.results)
}
```

## 4.3 Results and discussion


### 4.3.1 Performance on balanced datast 
Figure: scan a range of minslpit candidates

We can spot overfitting by plotting accuracy on test and learn data against a complexity parameter (in our case, minsplit). Scanning a large range of complexity parameter.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
tuning.results = tibble() 

for (mspl in seq(1,300,5)){
    tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data7.learn,  k_folds=5, minsplit=mspl, cp=cp_fixed ))
}
```

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
summ.results<-tuning.results %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

 p03 <- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = ac.test, group = f, colour = 'accuracy test')) + 
  geom_line(aes(x=minsplit, y = ac.learn,group = f, colour = 'accuracy learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","red")) +  ylim(.7,1)+ ylab("accuracy")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))

p04<- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = se.test, group = f, colour = 'sensitivity test')) + 
  geom_line(aes(x=minsplit, y = se.learn,group = f, colour = 'sensitivity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","green"))+  ylim(.7,1)+ ylab("sensitivity")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))

p05<-summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity test')) + 
  geom_line(aes(x=minsplit, y = sp.learn,group = f, colour = 'specificity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","blue"))+  ylim(.7,1)+ ylab("specificity")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))
  

#specificity VS sensitivity
# p06<-summ.results %>%  ggplot() + theme_minimal()+
#   geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity')) + 
#   geom_line(aes(x=minsplit, y = se.test,group = f, colour = 'sensitivity')) + 
#   geom_line(aes(x=minsplit, y = ac.test,group = f, colour = 'accuracy')) + 
#   #geom_line(aes(x=minsplit, y = sp.se, group = f, colour = 'sp/se')) + 
#   geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)
# p06

#TODO adjust colors: learning as dotted, colors according to acc, se, sp

```

```{r, fig.cap=paste("Experimental evaluation of accuracy, sensitivity and selectivity for a single decision tree on the balanced dataset y7"), error=FALSE, warning=FALSE, message=FALSE}
wrap_plots(p03,p04,p05, guides = 'collect')
```
sensitivity is the percentage of true postives out of all observations that are in the Top group, i.e. it is the ability of the model to correctly classify a company that is a top performer.
Simmetrcally, specificity is the percentage of true negatives out of all observations that are not in the Top group, i.e. it is the ability of the model to correctly classify a company that is not top performer.
In this example, a model with minsplit around 150 has a low sensitivity and a hig specificity, hence they misclassify companies that are in the top gropu, and corretly classify companies that are not.

Sensitivity and specificity are inversely related should be considered together, and the choiche for optimization depends on the needs of the project. In our example, if we need to optimize the model to achieve a high specificity, we should choose minsplit in the range 50 to 200. conversely, higher sensitivity can be achieved using larger values of minsplit, in the range 200-300.

2] The formula to determine specificity is the following:

Specificity=(True Negatives (D))/(True Negatives (D)+False Positives (B))

  Highly sensitive tests will lead to positive findings for patients with a disease, whereas highly specific tests will show patients without a finding having no disease.[ 

Now we can optimize the tree using a range of minsplit candndates from 20 to 120, with Auto Tune tree

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}

AutoTuneTree <- function(data.learn, data.test, k_folds_testing, k_folds_tuning, minsplit_candidates) {
   
    folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  #split data into folds
  data_f <- data.learn %>% sample_frac(1) %>% group_by(isTop) %>% 
    mutate(fold=rep(1:k_folds_testing, length.out=n())) %>% ungroup
 
  #outer cycle: testing
  for(i in 1:k_folds_testing){
    
    #select the fold to be used for testing
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    tuning.results = tibble()
    
    # inner cycle: tuning
    for (mspl in minsplit_candidates){
      tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data.learn,  k_folds=10, minsplit=mspl, cp=cp_fixed ))    }
    
    #select optimal value for minsplit
    summarytable  <- tuning.results %>% 
      group_by(minsplit)%>%
      summarise( ac.test = mean(ac.test), minsplit = minsplit) %>% 
      arrange(desc(ac.test))  
    mspl.star <- summarytable %>%  head(1) %>% select(minsplit) %>% pull

    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = mspl.star, cp = -1)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )
    #save results
    folds.results <-  folds.results %>%
      add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = mspl.star)
    }
  # sort fold results on descending values of ac.test and minsplit
  # fold.results[1] is the highest minsplit that produces the highest accuracy 
  return(folds.results)
}
```


```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
optimal.params.7 <- AutoTuneTree(data7.learn, data7.test, 
                               k_folds_testing=3, 
                               k_folds_tuning=3, 
                               minsplit_candidates=seq(200,300,1))

```

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}

optimal.minsplit.7  <- optimal.params.7 %>% head(1) %>% select(minsplit) %>% pull
# optimal.acc.learn.7 <- optimal.params.7 %>% head(1) %>% select(ac.learn) %>% pull
# optimal.acc.test.7  <- optimal.params.7 %>% head(1) %>% select(ac.test) %>% pull

simple.tree.7 <- rpart(  isTop~.,data7.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.7, 
                        cp = cp_fixed)

```



```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}


summ.results.7 <-optimal.params.7 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

data.to.plot.7 <-  tibble('indic'='', 'method'='', 'value'=0)%>% head(0)#empty tibble 
data.to.plot.7  <- data.to.plot.7 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y7 tree 1', value =  mean(summ.results.7$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y7 tree 1', value =  mean(summ.results.7$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y7 tree 1', value =  mean(summ.results.7$sp.test)))

p10 <- summ.results.7 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  ylim(.7,1)
  #geom_hline(yintercept = proportions$majority.class[1])
#### commento HORIZONTAL LINE AT THE LEVEL OF majority/minority CLASS
p10


```


```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
 optimal.params.7 <- AutoTuneTree(data7.learn, data7.test, 
                               k_folds_testing=3, 
                               k_folds_tuning=3, 
                               minsplit_candidates=seq(80,120,1))

optimal.minsplit.7  <- optimal.params.7 %>% head(1) %>% select(minsplit) %>% pull
# optimal.acc.learn.7 <- optimal.params.7 %>% head(1) %>% select(ac.learn) %>% pull
# optimal.acc.test.7  <- optimal.params.7 %>% head(1) %>% select(ac.test) %>% pull

summ.results.7 <-optimal.params.7 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

complex.tree.7 <- rpart(  isTop~.,data7.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.7, 
                        cp = cp_fixed)

data.to.plot.7  <- data.to.plot.7 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y7 tree 2', value =  mean(summ.results.7$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y7 tree 2', value =  mean(summ.results.7$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y7 tree 2', value =  mean(summ.results.7$sp.test)))


```


```{r, fig.cap=paste("A visual representation of the binary decision tree optimized for the imbalanced dataset y7")}
figures <- par(mfrow=c(1,2)) # more trees in the same figure
simple.tree.7  %>% rpart.plot(  main="A simple classification tree", shadow.col = "gray")
complex.tree.7 %>% rpart.plot(  main="A complex classification tree")
par(figures)
```




#### performance of random forrest
figure: boxplot A/S/S + other
comment increased performance, but exaliabe only for the first.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}

assesAccuracySensitivityScpecificityCaret <- function(train, test, ctrl, name, data.to.plot) {
  rf.model <- train(Class ~ .,
                    data = train,
                     positive = "other",
                    method = "rf",
                    verbose = FALSE,
                    metric = "ROC",
                    trControl = ctrl)
  
  ######################## TODO: relpace single test with a k-fold cross validation
  predicted.y <- predict(rf.model, newdata= test)

 conf.matrix <- table(predicted.y, test$Class)#confusion matirx

  true.neg  <- conf.matrix[2,2]
  true.pos  <- conf.matrix[1,1]
  false.neg <- conf.matrix[2,1]
  false.pos <- conf.matrix[1,2]
  pos = true.pos + false.neg
  neg = true.neg + false.pos
 
  data.to.plot  <- data.to.plot %>% 
    add_row( tibble_row(indic = 'accuracy',   method =  name,value =  (true.pos + true.neg) / (pos+neg) )) %>%
    add_row( tibble_row(indic = 'sensitivity',method =  name,value =  (true.pos/(true.pos+false.neg))  )) %>%
    add_row( tibble_row(indic = 'specificity',method =  name,value =  (true.neg/(true.neg+false.pos)) ))
  
  return(data.to.plot)
}

```


```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
require(caret)
data7r <- data7 %>% 
  mutate(Class = case_when(isTop== TRUE   ~ "top" , isTop== FALSE   ~ "other")) %>% 
  select(-isTop)  
train <- data7r %>% slice( indexes.learning)
test  <- data7r %>% slice(-indexes.learning) 
#check if classes are balanced
# prop.table(table(train$Class))
# prop.table(table(test$Class))

# general control function for training
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,#10
                     repeats = 1,#5
                     summaryFunction = twoClassSummary,
                     savePredictions = "final",
                     classProbs = TRUE)

data.to.plot.7 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl,
                                                          name='y7 RF', 
                                                          data.to.plot.7) 

ctrl$sampling <- "up" 
data.to.plot.7 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y7 RF UP', 
                                                          data.to.plot.7) 

ctrl$sampling <- "down" 
data.to.plot.7 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y7 RF DOWN', 
                                                          data.to.plot.7)

ctrl$sampling <- "smote" 
data.to.plot.7 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y7 RF SMOTE', 
                                                          data.to.plot.7) 



```

# ```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
# rf.model <- train(Class ~ .,
#                     data = train,
#                      positive = "other",
#                     method = "rf",
#                     verbose = FALSE,
#                     metric = "ROC",
#                     trControl = ctrl)
# rf.model$results$Sens# sensitivity on learning data
# 
# ```

```{r, fig.cap=paste("Experimantal assessment of accuracy, sensitivity and specificity for a random forrest model on a balanced dataset y7, using oversampling, undersampling and SMOTE"),  error=FALSE, warning=FALSE, message=FALSE}
data.to.plot.7 %>% 
  mutate(method1 = factor(method, levels = c('y7 tree 1', 'y7 tree 2',  'y7 RF', 'y7 RF UP', 'y7 RF DOWN', 'y7 RF SMOTE' ))) %>%
  ggplot()+ geom_line(aes(x = method1, y = value, group = indic, color = indic))+
  geom_point(aes(x = method, y = value, shape = method, color = indic))+
  geom_hline(yintercept=.8,linetype = 'dotted', col = 'black', size=1)+ xlab("")+
  #geom_hline(yintercept = proportions$majority.class[1])+
  theme(axis.text.x = element_text(angle = 90))  + theme(legend.position="right")+ylim(0.7,.9)

```


### 4.3.3 Performance on inbalanced datast 
Figure: optimal tree. similar to previous case
```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
tuning.results = tibble() 

for (mspl in seq(1,300,5)){
    tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data8.learn,  k_folds=5, minsplit=mspl, cp=cp_fixed ))
}
```

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
summ.results<-tuning.results %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

 p03 <- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = ac.test, group = f, colour = 'accuracy test')) + 
  geom_line(aes(x=minsplit, y = ac.learn,group = f, colour = 'accuracy learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","red")) +  ylim(0.5,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2],col = 'yellow')

p04<- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = se.test, group = f, colour = 'sensitivity test')) + 
  geom_line(aes(x=minsplit, y = se.learn,group = f, colour = 'sensitivity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","green"))+  ylim(0.5,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2],col = 'yellow')

p05<-summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity test')) + 
  geom_line(aes(x=minsplit, y = sp.learn,group = f, colour = 'specificity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","blue"))+  ylim(0.5,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2] ,col = 'yellow')
 

# #specificity VS sensitivity
# p06<-summ.results %>%  ggplot() + theme_minimal()+
#   geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity')) + 
#   geom_line(aes(x=minsplit, y = se.test,group = f, colour = 'sensitivity')) + 
#   geom_line(aes(x=minsplit, y = ac.test,group = f, colour = 'accuracy')) + 
#   #geom_line(aes(x=minsplit, y = sp.se, group = f, colour = 'sp/se')) + 
#   geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)


#TODO adjust colors: learning as dotted, colors according to acc, se, sp
 

```

```{r, fig.cap=paste("Experimental evaluation of accuracy, sensitivity and selectivity for a single decision tree on the inbalanced dataset y8")}
wrap_plots(p03,p04,p05, guides = 'collect')

```


sensitivity drops and accuracy surges:  it's the "accuracy paradox" in unbalanced datasets

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
optimal.params.8 <- AutoTuneTree(data8.learn, data8.test, 
                               k_folds_testing=3, k_folds_tuning=3, 
                               minsplit_candidates=seq(30,100,1))
```

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}

optimal.minsplit.8  <- optimal.params.8 %>% head(1) %>% select(minsplit) %>% pull
optimal.acc.learn.8 <- optimal.params.8 %>% head(1) %>% select(ac.learn) %>% pull
optimal.acc.test.8  <- optimal.params.8 %>% head(1) %>% select(ac.test) %>% pull

optimal.tree.8 <- rpart(  isTop~.,data8.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.8, 
                        cp = -1)
```

```{r, fig.cap=paste("A visual representation of the binary decision tree optimized fot the imbalanced dataset y8")}
rpart.plot(optimal.tree.8)



```
```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}


summ.results.8<-optimal.params.8 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

data.to.plot.8 <-  tibble('indic'='', 'method'='', 'value'=0)%>% head(0)#empty tibble 
data.to.plot.8  <- data.to.plot.8 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y8 tree', value =  mean(summ.results.8$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y8 tree', value =  mean(summ.results.8$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y8 tree', value =  mean(summ.results.8$sp.test)))

p11 <- summ.results.8 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("summary results for optimal tree")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0,1)
p11


```


text: performance of optimal tree. comment:
figure: boxplot A/S/S ,compare and comment: seems high but is misleading


text: performance of random forrest + OVERSAMPLING
figure: boxplot A/S/S + other
comment increased performance, but exaliabe only for the first.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
library(caret)
set.seed(123)

data8r <- data8 %>% 
  mutate(Class = case_when(isTop== TRUE   ~ "top" , isTop== FALSE   ~ "other")) %>% 
  select(-isTop)  
train <- data8r %>% slice( indexes.learning)
test  <- data8r %>% slice(-indexes.learning) 
# prop.table(table(train$Class))
# prop.table(table(test$Class))
# # general control function for training
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,#10
                     repeats = 3,#5
                     summaryFunction = twoClassSummary,
                     savePredictions = "final",
                     classProbs = TRUE)

data.to.plot.8 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl,
                                                          name='y8 RF', 
                                                          data.to.plot.8) 

ctrl$sampling <- "up" 
data.to.plot.8 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y8 RF UP', 
                                                          data.to.plot.8) 

ctrl$sampling <- "down" 
data.to.plot.8 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y8 RF DOWN', 
                                                          data.to.plot.8)

ctrl$sampling <- "smote" 
data.to.plot.8 <- assesAccuracySensitivityScpecificityCaret(train, test, ctrl, 
                                                          name='y8 RF SMOTE', 
                                                          data.to.plot.8) 

```



```{r, fig.cap=paste("Experimantal assessment of accuracy, sensitivity and specificity for a random forrest model on a inbalanced dataset y8, using oversampling, undersampling and SMOTE")}

### improve this figure: a facet for every model > A, Se, SP boxplot
data.to.plot.8 %>% 
  mutate(method1 = factor(method, levels = c('y8 tree', 'y8 RF', 'y8 RF UP', 'y8 RF DOWN', 'y8 RF SMOTE' ))) %>%
  ggplot()+ geom_line(aes(x = method1, y = value, group = indic, color = indic))+
  geom_point(aes(x = method, y = value, shape = method, color = indic))+
  geom_hline(yintercept=.8,linetype = 'dotted', col = 'black', size=1)+ xlab("")+
  #geom_hline(yintercept = proportions$majority.class[1])+
  theme(axis.text.x = element_text(angle = 90))  + theme(legend.position="right")#+ylim(0.7,.9)

```
### 4.3.5 Comparison of results

4 plots


# 5. Concluding remarks  

This notebook demonstrates that a machine learning algorithm can predict whether a company belongs to the group of "top performers", and that the quality of predictions depends strongly on the definition on the degree of imbalance of the  dataset. 

The performance has been demonstrated for a binary classification algorithm (namely a single tree, generated by the recursive partitioning algorithm of r-part library) and for a random forrest. The dataset used for learning and validation is composed of n = `r nrow(data)` observations and p = `r ncol(data)` features, obtained from a larger dataset of regional companies, filtered on NACE sectors of activity and on year 2019.

Two case studies have been examined in detailed, using different definitions of the y label (isTop8, a narrow definition addressing about 15% of the sample) and isTop7 (a balanced dataset addressing about half of the ocmpanies).

The model has been tuned to maximize accuracy on test data using a nested k-fold cross validation loops for tuning and assessing experimentally the performance, a procedure that gives good results for balanced datasets. The imbalanced datasets has been treated with a *random forrest* model and an oversampling of the minority class, to achieve a good accuracy, and improving on sensitivity and specificity.



In both cases computation time is of the order of minutes, which is in line with the constraints of the project.
the model is interpretable and explainable and the accuracy of predictions has been measued experimetnally. In 2 cases is above the threshold, and in another case is below the threshold accuracy > 80%. 

The quality of predictions depends on the selection of companies included in the dataset: we may expect that smaller, homogeneous sets may result in highre accuracy (e.g. selecting only companies of a given size, or of a single NACE sector) and the exact definition of the label. In future applications the model *should be learned for specific subsets and specific definitions of label*, and accuracy (or quality of predictions) has to be evaluated for each case. 
 

Preliminary results suggest further investigations: 

- how data on employment (staff turnover, balance and stock) influence the quality of predictions. Such data is not available for companies outside FVG.
- robustness with respect to differet definition of label y isTop. If we use a different definition to rating =8 or ==7...

- performance with other supevised models: multiclass decision trees, random forrest, svm, naive bayes, knn 

- unsupervised: PCA on all variables, intrinsic dimension and k-means clustering
 
- explore time-dependent analysis: eg how data from 2018 can predict perfomance in 2019, and mode in depth in 2020 (where a disruptive change in economy took place).

# References
[1] www.areasciencepark.it
[2] www.innovationintelligence.it
[3] https://www.modefinance.com/en/data-science 
[4] european nace codes 
[5] Italian Business Register 
[5] ISL
[6] Wu, J., et al. Credit Rating Prediction Through Supply Chains: A Machine Learning Approach. Production and Operations Management (2021), https://doi.org/10.1111/poms.13634
[7]
 
