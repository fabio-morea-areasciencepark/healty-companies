--- 
title: "Machine Learning project: \n Annexes"
author: "Fabio Morea"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    fig_width: 6
    fig_height: 3
  
---

This notebook contains the ANNEXES of the final project for the course *introduction to machine learning*.
Before executing the main notebook, running the Annexes will load relevant libraries, datasets and functions. 



# Annex 1: Libraries 

The notebook has been written using *R-Studio*; data manipulation is based on *tidyverse* [www.tidyverse.org/], a data science library that includes *magrittr* (pipe operator %>%), *dplyr* (select, summarize...), *tibble* (a tidier version of the data.frame) and *ggplot2* (visualizations).

```{r , error=FALSE, warning=FALSE, message=FALSE}
require(tidyverse)
require(ggthemes)
require(patchwork)
require(ggcorrplot)  
theme_set( theme_hc(base_size = 12))  
```

Original and tidy data are updated on a monthly basis; the current version in based on June 2021 version and does not provide automatic updating. 


\newpage


The graphical representation of decision trees will be generated using `rpart.plot` library. 

```{r , error=FALSE, warning=FALSE, message=FALSE}
require(rpart)
require(rpart.plot)
```




# Annex 2: Data acquisition and feature engineering

## data sources and pre processing

```{r , echo = FALSE}
pathTidyData = './../../_data/tidy/'
```

The data available from Innovation Intelligence needs to be pre-processed in order to obtain a *tidy* dataset suitable for ML. After pre-processing, the data fulfills the following requirements:

- encoded in UTF-8, cleaned from non-printable characters
- table columns are attributes (features, independent variables), renamed to be human- and machine-readable
- table rows are observations  If you have multiple tables, they should include a column in the table that allows them to be linked
- splitted into several tables, created unique identifiers to connect the tables
- saved each table to separate .csv file with a hunam-readable name.

No attributes were removed or summarized during pre-processing. Pre-processing is described in a separate notebook, providing details on all the attributes available in the raw data, and the transformations used to produce a smaller, cleaner data set ready for further analysis. Tidy data is saved in local folder *data/tidy*. 

## Data Acquisition 

The first task in feature engineering is the selection of relevant datasets and features, based on domanin knowledge. The features that may have a predictive power on financial rating are "financial indicators" from the official balance sheet of each company, as well as some categorical attributes (is a startup, an "innovative SME", a "young" of "women-led companies" ). 

In this case the relevant datasets are available in the following files: 
-  *cmp.csv* and *codes.csv*: company information from the Italian Business Registry. Each observation is a company, there are p = 41 attributes. The study will be focused on a subset filter companies that belong to a specific sector () and of a specific type.
-  *bsd.csv*. Each observation is a summary of balance sheet data (bsd) of a company (identified by *cf*) for a given year. Column labels need some improvement to remove whitespaces and possibly short english names.
- *rating.csv*. The financial rating of each company. 
- *employees*. Stock and flows of employees. *empl-flow.csv* and *empl-stock.csv*. 
Data is loaded in separate data structures (tibbles) 

```{r , error=FALSE, warning=FALSE, message=FALSE}
companies  <- read_csv( paste0(pathTidyData,"cmp.csv"),       show_col_types = FALSE ) 
bsd        <- read_csv( paste0(pathTidyData,"bsd.csv"),       show_col_types = FALSE ) 
rating     <- read_csv( paste0(pathTidyData,"rating.csv"),    show_col_types = FALSE ) 
codes      <- read_csv( paste0(pathTidyData,"nace.csv"),      show_col_types = FALSE )
empl.flows <- read_csv( paste0(pathTidyData,"empl_flows.csv"),show_col_types = FALSE )
empl.stock <- read_csv( paste0(pathTidyData,"empl_stock.csv"),show_col_types = FALSE )
```

### Further information on NACE CODES and Company Types
The sample is selected according to **NACE codes** and **company type**. The first selection on company type : we select all types that have a duty of disclosure of financial information, and therefore are suitable for the analysis, namely SU (società a responsabilità limitata con unico socio), SR (società a responsabilità limitata), SP (società per azioni), SD (società europea),  RS (società a responsabilità limitata semplificata), RR (società a responsabilità limitata a capitale ridotto), AU (società  per azioni con socio unico), AA (società in accomandita per azioni.

```{r, error=FALSE, warning=FALSE, message=FALSE}
selectedNg = c("SU", "SR", "SP", "SD", "RS", "RR", "AU", "AA")
companies <- companies %>% filter(ng2 %in% selectedNg)
```

A further selection is based on **NACE codes**. The acronym NACE stands for *Nomenclature of Economic Activities*, a standard classification for classifying business activities managed by EUROSTAT and recognized by national statistic offices at European level. NACE codes provide a framework for the collection and presentation of a wide range of statistics in economic fields such as production, employment, national accounts, and others. The statistics produced on the basis of NACE codes are comparable at the European level and more generally at the global level. The use of NACE codes is compulsory within the European statistics system. (see _data/ino/ for a complete list of codes https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/-/ks-ra-07-015

The NACE code is subdivided into a hierarchical structure with four levels: 

- Level 1: 21 sections identified by alphabetical letters A to U;
- Level 2: 88 divisions identified by two-digit numerical codes (01 to 99);
- Level 3: 272 groups identified by three-digit numerical codes (01.1 to 99.0);
- Level 4: 615 classes identified by four-digit numerical codes (01.11 to 99.00).

Each company can be associated with one or more NACE codes, that may be different for each local unit, and are identified as main (I), "primary" (P), or "Ancillary" (S). The selected sample is composed of companies that have at least one NACE code in one of the following Divisions: 22 (Manufacture of rubber and plastic products), 23 (Manufacture of other non-metallic mineral products), 24 (Manufacture of basic metals), 25 (Manufacture of fabricated metal products, except machinery and equipment), 26 (Manufacture of computer, electronic and optical products), 27 (Manufacture of electrical equipment) and 28 (Manufacture of machinery and equipment).

Some filters are applied: time-dependent data (such as balaance sheet data, rating and employees folws) are filtered to year 2019, and company age (years in business) is filtered to anly value greater than 1.

### Additional information on balance sheet data

- totAssess: totale attivo = total assets
- noi: RON Reddito Operativo Nnetto = NOI Net Operating Income
- personnel: totale costi del personale = total personnel costs
- debts: debiti esigibili entro l'esercizio successivo = debts due within the following financial year
- totEquity: totale patrimonio netto = total equity
- profLoss: utile/perdita esercizio ultimi = profit / loss for the last financial year
- accounts: crediti esigibili entro l'esercizio successivo = accounts receivables
- totIntang: totale immobilizzazioni immateriali = total intangible fixed assets
- prod: totale valore della produzione = total production value
- revenues: ricavi delle vendite = revenues from sales
- valCost: differenza tra valore e costi della produzione = difference between production value and production costs
- ammort: ammortamento immobilizzazione immateriali = amortisation
- valAdded: valore aggiunto = value added
- deprec: tot.aam.acc.svalutazioni = total amortisation, depreciation and write-downs


### Additional information on financial rating

Financial rating is a numerical variable ranging from 1 to 10, where low values denote an insufficient capability to meet financial obligations, and high values denote very good or excellent reliability. The value is generated by a prorpietary algorithm summarizing the overall performance of a company in all its economic and financial areas: profitability, liquidity, solvency, efficiency, production. 
Only a part of the data used to generate financial ratings is available for this project, thus we may expect that the actual value will be hard to predict. According to some similar cases described in literature [7, 8] reasonably good predictions of rating can be based on public financial indicators (balance sheet data). In our case we can and information on employees (number of employees, turnover and net balance in a given year), that are a good proxy for company health and performance.

```{r, error=FALSE, warning=FALSE, message=FALSE}
#NACE codes filter
divs = c( 22,23,24,25,26,27,28)
selectedCf <- codes %>% filter(division %in% divs) %>% select(cf) 

#joining tibbles: semi_join() returns all rows from first table with a match in second table
companies  <- companies %>% semi_join(selectedCf) %>% filter(yearsInBusiness > 1)
bsd        <- bsd       %>% semi_join(selectedCf) %>% filter(year == 2019) 
rating     <- rating    %>% semi_join(selectedCf) %>% filter(year == 2019) 

#preparing data on employees turnover and balance, summarized by company
empl.flows <- empl.flows%>% semi_join(selectedCf) %>% filter(year == 2019) %>%
  group_by(cf)%>% summarise(staffTurnover = sum(turnover), staffBalance = sum(balance))

# preparing data on employees stock summarized by company and filtered [1..999]
empl.stock <- empl.stock%>% semi_join(selectedCf) %>%
  group_by(cf)%>% summarise(StockAll=max(StockAll)) %>%
  filter(StockAll > 0) %>% filter(StockAll < 1000) 
```

The feature matrix *X* can be created by joining the tibbles on a company identifier (cf), selecting relevant features and mutating boolean variables into factors. 

```{r error=FALSE, warning=FALSE, message=FALSE}
companies <- companies %>% 
       inner_join(bsd, by = "cf") %>% 
       inner_join(rating, by = "cf") %>% 
       inner_join(empl.flows, by = "cf") %>% 
       inner_join(empl.stock, by = "cf")   

X <- companies %>% select(idCompany, is.sme, is.startup, is.fem, is.young,  
  yearsInBusiness,staffTurnover, staffBalance, StockAll, 
  totAssets, totEquity, totIntang,  accounts, debts,ammort, deprec,prod,revenues, 
  personnel,  valCost,profLoss, valAdded, noi) %>%
  mutate(is.sme = factor(is.sme)) %>%
  mutate(is.startup = factor(is.startup)) %>%
  mutate(is.fem  = factor(is.fem)) %>%
  mutate(is.young = factor(is.young)) %>% 
  mutate(idCompany = factor(idCompany)) %>% na.omit()

```


## Feature engineering

The objective of feature engineering is to build a dataset that contains a set of relevant variables for learning and prediction, appropriately scaled, in the form of a matrix. 
Specifically we will focus on calculating new features based on domain knowledge, checking variable correlation and normalizing the selected features by centering and scaleing.

The original features are higly correlated, as higlighted in the following correlation matrix.  
```{r, echo=FALSE}
cm <- X %>% select(where(is.numeric), -idCompany) %>% cor() 
p.fe.1 <-ggcorrplot(cm,  type = "lower",insig = "blank") +
  theme(axis.text.x=element_text(size=10, angle=90))
p.fe.1
```

Moreover, featur values range over different orders of magnitude (company age ranges from 1 to 150, while total assets ranges from 0 to $10^9$ €).

```{r echo=FALSE}
X <- X %>% select(idCompany, totAssets,totEquity, noi,personnel,prod, debts,deprec,valCost,totIntang, revenues,valAdded,yearsInBusiness, staffTurnover, staffBalance, StockAll) %>% na.omit()
p.fe.2 <- ggplot(stack(X), aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="original features") +
 geom_boxplot(fill = "white", colour = "black") + coord_flip()
p.fe.2
```



Keeping only the new features, the dataset consists of n = r nrow(data) observations and p = r ncol(data) features (namely: `r names(data)` ). 

The next step is to normalize (center and rescale) numeric features to the a similar range in order to improve the performance of the learning algorithm. 


All features are of the same order of magnitude. The dataset is composed of n = r nrow(data) observations and p = r ncol(data) features (namely: `r names(data)` ). There meaning of variables is  self explanatory in some cases, but economic features may require an explanation:  



```{r, echo = FALSE}
X %>% write.csv(file = paste0(pathTidyData,"X.csv"),row.names = FALSE) #save data for future applications
```
 
### Labels based on financial ratings

Ratings are available for years 2018, 2019 and 2020; for the purpose of this study a single year (2019) will be selected. In order to tackle a more manageable problem, the prediction will not be focused on the specific value of financial rating, but will be aggregated in 2 classes: Top8 (financial rating >=  8, a sTrongly imbalanced dataset representing generally aroung 20% of the companies) and Top7 (financial rating >= 7, a balanced distribution representing generally half of the companies.

In order to assess the performance of a decision tree under a variety of conditions, we need to build different y labels, appropriate for a binary classification (in the form of a factor variable in R).

Three classes of interest are defined below: isTop9 (a narrow class of top-rating companies), isTop8 (about one quarter of the companies) and isTop7 (above-average companies). The choice of classes highligts a relavant issue in classification: imbalance in the distirbution of labele. In the folloqing, the underrepresented class will be referredd to as *minority class*, and the over represented class is referred to as *majority class*.

```{r error=FALSE, warning=FALSE, message=FALSE}
ys <-companies %>% 
  mutate(isTop8 = (rating010 >8))  %>% 
  mutate(isTop7 = (rating010 >=7))  %>%  
  mutate(idCompany = as.factor(idCompany))%>%
  select(idCompany, isTop8, isTop7)
```



```{r, echo=FALSE} 
#saving dataset for future use
ys %>%  write.csv(file = paste0(pathTidyData,"y.csv"),row.names = FALSE) #save data for future applications
```

```{r, echo=FALSE}
proportions <- ys %>% pivot_longer(cols=c( isTop8, isTop7)) %>% 
  group_by(name) %>% 
  summarise(minority.class = round(mean(value),3)) %>%  
  mutate(majority.class = 1 - minority.class) 
#print(tbl_df(proportions))
knitr::kable(proportions)
 
```



Annex 4:  Examples of Binary Classification Trees

The proposed solution is to learn "Binary Classification Tree" and tune the model to achieve the expcted performance. The solution can be implemented using the two usual blocks:
- a function for learning the model `rpart()`
- a function for prediction `predict()`

The learning function in `rpart()` requires data in the form of a matrix (a data.frame or a tibble) and instructions on which features should be predicted, expressed as a *formula*. In our case the formula used for all trees is  `isTop ~ .` which stands for "*feature `isTop` depends on all the other variables*".

In this phase we will focus on the label `isTop8` (other labels will be used in section 5 for comparison).

```{r, error=FALSE, warning=FALSE, message=FALSE}
y7 <- ys %>% mutate(isTop = as.factor(isTop7)) %>%select(idCompany,isTop)
y8 <- ys %>% mutate(isTop = as.factor(isTop8)) %>%select(idCompany,isTop)
```

The `rpart` library requires a dataset in the form of a single table, that can be obtained joining X and y. The company identifier can be removed at this stage, since it carries no information on the company and may only lead to overfitting. 

```{r , error=FALSE, warning=FALSE, message=FALSE}
# data <- X %>% 
#   inner_join(y7) %>% 
#   select(-idCompany)
```
# PCA
https://www.datacamp.com/community/tutorials/pca-analysis-r

```{r}
X1 <- X %>% filter(complete.cases(.)) %>%  head(20)

principal.components <- prcomp(X1, center = TRUE,scale. = TRUE)

require(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)


ggbiplot(principal.components)
ggbiplot(principal.components, labels=rownames(X1), ellipse=TRUE,circle=TRUE)

eigs <- principal.components$sdev^2
explained.variance <- eigs/ sum(eigs)
summary(principal.components)

principal.components

explained.variance


ggplot(data.frame(explained.variance),aes(seq_along(explained.variance),explained.variance))+
  geom_bar(stat="identity")+ylim(0,1)


eigs

```
```{r}
https://www.datacamp.com/community/tutorials/pca-analysis-r
```

 
