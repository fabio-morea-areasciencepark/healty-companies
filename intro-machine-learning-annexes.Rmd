--- 
title: "Machine Learning project: \n Annexes"
author: "Fabio Morea"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    fig_width: 6
    fig_height: 3
  
---

This notebook contains the ANNEXES of the final project for the course *introduction to machine learning*.
Before executing the main notebook, running the Annexes will load relevant libraries, datasets and functions. 



# Annex 1: Libraries 

The notebook has been written using *R-Studio*; data manipulation is based on *tidyverse* [www.tidyverse.org/], a data science library that includes *magrittr* (pipe operator %>%), *dplyr* (select, summarize...), *tibble* (a tidier version of the data.frame) and *ggplot2* (visualizations).


```{r , error=FALSE, warning=FALSE, message=FALSE}
require(tidyverse)
require(ggthemes)
require(patchwork)
require(ggcorrplot)  
theme_set( theme_hc(base_size = 12))  
```

Original and tidy data are updated on a monthly basis; the current version in based on June 2021 version and does not provide automatic updating. 


\newpage


The graphical representation of decision trees will be generated using `rpart.plot` library. 

```{r , error=FALSE, warning=FALSE, message=FALSE}
require(rpart)
require(rpart.plot)
```




# Annex 2: Data acquisition and feature engineering

## data sources and pre processing

```{r , echo = FALSE}
pathTidyData = './../../_data/tidy/'
```

The data available from Innovation Intelligence needs to be pre-processed in order to obtain a *tidy* dataset suitable for ML. After pre-processing, the data fulfills the following requirements:

- encoded in UTF-8, cleaned from non-printable characters
- table columns are attributes (features, independent variables), renamed to be human- and machine-readable
- table rows are observations  If you have multiple tables, they should include a column in the table that allows them to be linked
- splitted into several tables, created unique identifiers to connect the tables
- saved each table to separate .csv file with a hunam-readable name.

No attributes were removed or summarized during pre-processing. Pre-processing is described in a separate notebook, providing details on all the attributes available in the raw data, and the transformations used to produce a smaller, cleaner data set ready for further analysis. Tidy data is saved in local folder *data/tidy*. 

## Data Acquisition 

The first task in feature engineering is the selection of relevant datasets and features, based on domanin knowledge. The features that may have a predictive power on financial rating are "financial indicators" from the official balance sheet of each company, as well as some categorical attributes (is a startup, an "innovative SME", a "young" of "women-led companies" ). 

In this case the relevant datasets are available in the following files: 
-  *cmp.csv* and *codes.csv*: company information from the Italian Business Registry. Each observation is a company, there are p = 41 attributes. The study will be focused on a subset filter companies that belong to a specific sector () and of a specific type.
-  *bsd.csv*. Each observation is a summary of balance sheet data (bsd) of a company (identified by *cf*) for a given year. Column labels need some improvement to remove whitespaces and possibly short english names.
- *rating.csv*. The financial rating of each company. 
- *employees*. Stock and flows of employees. *empl-flow.csv* and *empl-stock.csv*. 
Data is loaded in separate data structures (tibbles) 

```{r , error=FALSE, warning=FALSE, message=FALSE}
companies  <- read_csv( paste0(pathTidyData,"cmp.csv"),       show_col_types = FALSE ) 
bsd        <- read_csv( paste0(pathTidyData,"bsd.csv"),       show_col_types = FALSE ) 
rating     <- read_csv( paste0(pathTidyData,"rating.csv"),    show_col_types = FALSE ) 
codes      <- read_csv( paste0(pathTidyData,"nace.csv"),      show_col_types = FALSE )
empl.flows <- read_csv( paste0(pathTidyData,"empl_flows.csv"),show_col_types = FALSE )
empl.stock <- read_csv( paste0(pathTidyData,"empl_stock.csv"),show_col_types = FALSE )
```

### Further information on NACE CODES and Company Types
The sample is selected according to **NACE codes** and **company type**. The first selection on company type : we select all types that have a duty of disclosure of financial information, and therefore are suitable for the analysis, namely SU (società a responsabilità limitata con unico socio), SR (società a responsabilità limitata), SP (società per azioni), SD (società europea),  RS (società a responsabilità limitata semplificata), RR (società a responsabilità limitata a capitale ridotto), AU (società  per azioni con socio unico), AA (società in accomandita per azioni.

```{r, error=FALSE, warning=FALSE, message=FALSE}
selectedNg = c("SU", "SR", "SP", "SD", "RS", "RR", "AU", "AA")
companies <- companies %>% filter(ng2 %in% selectedNg)
```

A further selection is based on **NACE codes**. The acronym NACE stands for *Nomenclature of Economic Activities*, a standard classification for classifying business activities managed by EUROSTAT and recognized by national statistic offices at European level. NACE codes provide a framework for the collection and presentation of a wide range of statistics in economic fields such as production, employment, national accounts, and others. The statistics produced on the basis of NACE codes are comparable at the European level and more generally at the global level. The use of NACE codes is compulsory within the European statistics system. (see _data/ino/ for a complete list of codes https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/-/ks-ra-07-015

The NACE code is subdivided into a hierarchical structure with four levels: 

- Level 1: 21 sections identified by alphabetical letters A to U;
- Level 2: 88 divisions identified by two-digit numerical codes (01 to 99);
- Level 3: 272 groups identified by three-digit numerical codes (01.1 to 99.0);
- Level 4: 615 classes identified by four-digit numerical codes (01.11 to 99.00).

Each company can be associated with one or more NACE codes, that may be different for each local unit, and are identified as main (I), "primary" (P), or "Ancillary" (S). The selected sample is composed of companies that have at least one NACE code in one of the following Divisions: 22 (Manufacture of rubber and plastic products), 23 (Manufacture of other non-metallic mineral products), 24 (Manufacture of basic metals), 25 (Manufacture of fabricated metal products, except machinery and equipment), 26 (Manufacture of computer, electronic and optical products), 27 (Manufacture of electrical equipment) and 28 (Manufacture of machinery and equipment).

Some filters are applied: time-dependent data (such as balaance sheet data, rating and employees folws) are filtered to year 2019, and company age (years in business) is filtered to anly value greater than 1.

### Additional information on balance sheet data

- totAssess: totale attivo = total assets
- noi: RON Reddito Operativo Nnetto = NOI Net Operating Income
- personnel: totale costi del personale = total personnel costs
- debts: debiti esigibili entro l'esercizio successivo = debts due within the following financial year
- totEquity: totale patrimonio netto = total equity
- profLoss: utile/perdita esercizio ultimi = profit / loss for the last financial year
- accounts: crediti esigibili entro l'esercizio successivo = accounts receivables
- totIntang: totale immobilizzazioni immateriali = total intangible fixed assets
- prod: totale valore della produzione = total production value
- revenues: ricavi delle vendite = revenues from sales
- valCost: differenza tra valore e costi della produzione = difference between production value and production costs
- ammort: ammortamento immobilizzazione immateriali = amortisation
- valAdded: valore aggiunto = value added
- deprec: tot.aam.acc.svalutazioni = total amortisation, depreciation and write-downs


### Additional information on financial rating

Financial rating is a numerical variable ranging from 1 to 10, where low values denote an insufficient capability to meet financial obligations, and high values denote very good or excellent reliability. The value is generated by a prorpietary algorithm summarizing the overall performance of a company in all its economic and financial areas: profitability, liquidity, solvency, efficiency, production. 
Only a part of the data used to generate financial ratings is available for this project, thus we may expect that the actual value will be hard to predict. According to some similar cases described in literature [7, 8] reasonably good predictions of rating can be based on public financial indicators (balance sheet data). In our case we can and information on employees (number of employees, turnover and net balance in a given year), that are a good proxy for company health and performance.

```{r, error=FALSE, warning=FALSE, message=FALSE}
#NACE codes filter
divs = c( 22,23,24,25,26,27,28)
selectedCf <- codes %>% filter(division %in% divs) %>% select(cf) 

#joining tibbles: semi_join() returns all rows from first table with a match in second table
companies  <- companies %>% semi_join(selectedCf) %>% filter(yearsInBusiness > 1)
bsd        <- bsd       %>% semi_join(selectedCf) %>% filter(year == 2019) 
rating     <- rating    %>% semi_join(selectedCf) %>% filter(year == 2019) 

#preparing data on employees turnover and balance, summarized by company
empl.flows <- empl.flows%>% semi_join(selectedCf) %>% filter(year == 2019) %>%
  group_by(cf)%>% summarise(staffTurnover = sum(turnover), staffBalance = sum(balance))

# preparing data on employees stock summarized by company and filtered [1..999]
empl.stock <- empl.stock%>% semi_join(selectedCf) %>%
  group_by(cf)%>% summarise(StockAll=max(StockAll)) %>%
  filter(StockAll > 0) %>% filter(StockAll < 1000) 
```

The feature matrix *X* can be created by joining the tibbles on a company identifier (cf), selecting relevant features and mutating boolean variables into factors. 

```{r error=FALSE, warning=FALSE, message=FALSE}
companies <- companies %>% 
       inner_join(bsd, by = "cf") %>% 
       inner_join(rating, by = "cf") %>% 
       inner_join(empl.flows, by = "cf") %>% 
       inner_join(empl.stock, by = "cf")   

X <- companies %>% select(idCompany, is.sme, is.startup, is.fem, is.young,  
  yearsInBusiness,staffTurnover, staffBalance, StockAll, 
  totAssets, totEquity, totIntang,  accounts, debts,ammort, deprec,prod,revenues, 
  personnel,  valCost,profLoss, valAdded, noi) %>%
  mutate(is.sme = factor(is.sme)) %>%
  mutate(is.startup = factor(is.startup)) %>%
  mutate(is.fem  = factor(is.fem)) %>%
  mutate(is.young = factor(is.young)) %>% 
  mutate(idCompany = factor(idCompany)) %>% na.omit()

```


## Feature engineering

The objective of feature engineering is to build a dataset that contains a set of relevant variables for learning and prediction, appropriately scaled, in the form of a matrix. 
Specifically we will focus on calculating new features based on domain knowledge, checking variable correlation and normalizing the selected features by centering and scaleing.

The original features are higly correlated, as higlighted in the following correlation matrix.  
```{r, echo=FALSE}
cm <- X %>% select(where(is.numeric), -idCompany) %>% cor() 
p.fe.1 <-ggcorrplot(cm,  type = "lower",insig = "blank") +
  theme(axis.text.x=element_text(size=10, angle=90))
p.fe.1
```

Moreover, featur values range over different orders of magnitude (company age ranges from 1 to 150, while total assets ranges from 0 to $10^9$ €).

```{r echo=FALSE}
X <- X %>% select(idCompany, totAssets,totEquity, noi,personnel,prod, debts,deprec,valCost,totIntang, revenues,valAdded,yearsInBusiness, staffTurnover, staffBalance, StockAll) %>% na.omit()
p.fe.2 <- ggplot(stack(X), aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="original features") +
 geom_boxplot(fill = "white", colour = "black") + coord_flip()
p.fe.2
```



Keeping only the new features, the dataset consists of n = r nrow(data) observations and p = r ncol(data) features (namely: `r names(data)` ). 

The next step is to normalize (center and rescale) numeric features to the a similar range in order to improve the performance of the learning algorithm. 


All features are of the same order of magnitude. The dataset is composed of n = r nrow(data) observations and p = r ncol(data) features (namely: `r names(data)` ). There meaning of variables is  self explanatory in some cases, but economic features may require an explanation:  



```{r, echo = FALSE}
X %>% write.csv(file = paste0(pathTidyData,"X.csv"),row.names = FALSE) #save data for future applications
```
 
### Labels based on financial ratings

Ratings are available for years 2018, 2019 and 2020; for the purpose of this study a single year (2019) will be selected. In order to tackle a more manageable problem, the prediction will not be focused on the specific value of financial rating, but will be aggregated in 2 classes: Top8 (financial rating >=  8, a sTrongly imbalanced dataset representing generally aroung 20% of the companies) and Top7 (financial rating >= 7, a balanced distribution representing generally half of the companies.

In order to assess the performance of a decision tree under a variety of conditions, we need to build different y labels, appropriate for a binary classification (in the form of a factor variable in R).

Three classes of interest are defined below: isTop9 (a narrow class of top-rating companies), isTop8 (about one quarter of the companies) and isTop7 (above-average companies). The choice of classes highligts a relavant issue in classification: imbalance in the distirbution of labele. In the folloqing, the underrepresented class will be referredd to as *minority class*, and the over represented class is referred to as *majority class*.

```{r error=FALSE, warning=FALSE, message=FALSE}
ys <-companies %>% 
  mutate(isTop8 = (rating010 >8))  %>% 
  mutate(isTop7 = (rating010 >=7))  %>%  
  mutate(idCompany = as.factor(idCompany))%>%
  select(idCompany, isTop8, isTop7)
```



```{r, echo=FALSE} 
#saving dataset for future use
ys %>%  write.csv(file = paste0(pathTidyData,"y.csv"),row.names = FALSE) #save data for future applications
```

```{r, echo=FALSE}
proportions <- ys %>% pivot_longer(cols=c( isTop8, isTop7)) %>% 
  group_by(name) %>% 
  summarise(minority.class = round(mean(value),3)) %>%  
  mutate(majority.class = 1 - minority.class) 
#print(tbl_df(proportions))
knitr::kable(proportions)
 
```



Annex 4:  Examples of Binary Classification Trees

The proposed solution is to learn "Binary Classification Tree" and tune the model to achieve the expcted performance. The solution can be implemented using the two usual blocks:
- a function for learning the model `rpart()`
- a function for prediction `predict()`

The learning function in `rpart()` requires data in the form of a matrix (a data.frame or a tibble) and instructions on which features should be predicted, expressed as a *formula*. In our case the formula used for all trees is  `isTop ~ .` which stands for "*feature `isTop` depends on all the other variables*".

In this phase we will focus on the label `isTop8` (other labels will be used in section 5 for comparison).

```{r, error=FALSE, warning=FALSE, message=FALSE}
y7 <- ys %>% mutate(isTop = as.factor(isTop7)) %>%select(idCompany,isTop)
y8 <- ys %>% mutate(isTop = as.factor(isTop8)) %>%select(idCompany,isTop)
```

The `rpart` library requires a dataset in the form of a single table, that can be obtained joining X and y. The company identifier can be removed at this stage, since it carries no information on the company and may only lead to overfitting. 

```{r , error=FALSE, warning=FALSE, message=FALSE}
data <- X %>% 
  inner_join(y) %>% 
  select(-idCompany)
```


## Predictive power of features 
Features have different same predictive power on y label. The decision tree will provide a detailed estimate of each variable, but at this stage we can have a basic idea of the predictive power of each variable by examining the distributions of observations according to the label

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
p.fe.5 <- data %>%
  select(relEquity, relNoi, logStaff, VAS,yearsInBusiness,isTop)%>% 
  pivot_longer(cols=!isTop) %>% ggplot(aes(x=isTop, y=value, color = isTop)) +
  geom_boxplot() + facet_wrap(~name, ncol = 6) + 
  scale_y_continuous(  limit = c(-3, 3))
```

Some features, namely *relNoi* (net operative income scaled to total assets), relEquity (total equity scaled to total assets) and VAS (Value Added on total Staff), are clearly separate according to label y9 and can therefore be expected to be good predictors.

# Annex 5: tuning binary classification trees

The model complexity and performance will be controlled by a single parameter, minsplit, i.e. the minimum number of observations that must exist in a node in order for a split to be attempted. A high value of minsplit produces simple trees with large terminal nodes (example on the left) while low values of minsplit produces complex trees (example on the right).
Optimal performance can be achieved selecting an appropriate value of minsplit that produces the best results on test data. Such value will be determined experimentally in the next section of the notebook.

```{r}
#building a simple tree
parms = rpart.control(split = "gini", minsplit = 200 , cp=.01)
simple.tree <- rpart(isTop~., data, method = "class", control=parms)

#building a complex tree
parms = rpart.control(split = "gini",  minsplit = 5, cp=.005)
complex.tree <- rpart(isTop~., data, method = "class",control=parms)
```

```{r, echo = FALSE}
figures <- par(mfrow=c(1,2)) # more trees in the same figure
simple.tree %>% rpart.plot(  main="A simple classification tree", shadow.col = "gray")
complex.tree %>% rpart.plot(  main="A complex classification tree")
par(figures)
```
The models exhibit different complexity, accuracy and explainability.

## assessment of variable importance 
The recursive partitioning algorithm calculates the relative importance of each variable to achieve the final predictions. Values vary with data and hyperparameters.

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
 tibble(features=names(simple.tree$variable.importance) , imp=simple.tree$variable.importance) %>%
  mutate(imp = round(imp,1))%>%
  ggplot(aes(reorder(features, imp, sum), imp))  + coord_flip()+
  labs(x="", y="variable importance") +
  geom_bar(stat="identity", fill="light gray")+ 
  geom_text(aes(label=imp))
```

In the case of simple.tree model, the most relevant variables are relEquity, logStaff and relNoi.
Selecting the most predictive variables improves the performance of the algorithm. Selecting predictors with low predictive power can lead, in fact, to overfitting or low model performance. 

# Annex 6: functions to evaluate the performance of regression trees   

## Quality of prediction of a single tree

The quality of prediction of a single decision tree can be assessed using different parameters based on confusion matrixsuch: error rate, accuracy, specificity and sensitivity. 
A **confusion matrix** is a table (2x2 in the case of binary classifiers) that shows the number of values classified correctly (on the main diagonal) or misclassified (off-diagonal). 

```{r, error=FALSE, warning=FALSE, message=FALSE}
computeConfusionMatrix = function(model, data) {
  predicted.y = predict(model, data, type="class")
  return(table(predicted.y, data$isTop))
}
```

Learning and prediction are based a stocastic process (splitting data into learn and test sets), hence the results vary for each sample. In order to evaluate te average performance of the method, we need to assess it over a number of samples using a structured experimental procedure such as k-fold cross validation. 
The first step is to build a function that computes the accuracy:

```{r, error=FALSE, warning=FALSE, message=FALSE}
computeAccuracySensitivitySpecificity =  function(model, data) {
  conf.matrix <- computeConfusionMatrix (model, data)
  true.neg  <- conf.matrix[1,1]
  false.neg <- conf.matrix[1,2]
  false.pos <- conf.matrix[2,1]
  true.pos  <- conf.matrix[2,2]
  pos = true.pos + false.neg
  neg = true.neg + false.pos
  accuracy <- (true.pos + true.neg) / (pos+neg)
  sensitivity <- 1-(false.pos/neg) # Sensitivity: The “true positive rate” 
  specificity <- 1-(false.neg/pos)# Specificity: The “true negative rate”
  return( c(accuracy, sensitivity, specificity) )
}
```

Accuracy of the the `simple.tree` can be assessed as in the following example: the tree is trained on data.learn, then performance is assessed on data.test. 

```{r, error=FALSE, warning=FALSE, message=FALSE}
#split data into learn and test
p.learn = .8
indexes.learning = sample(c(1:nrow(data)))[1:(nrow(data)*p.learn)]
data.learn = data[ indexes.learning,]
data.test =  data[-indexes.learning,]

#learn simple tree on data.learn
parms = rpart.control(split = "gini", minsplit = 200 , cp=.01)
simple.tree <- rpart(isTop~., data.learn, method = "class", control=parms)

print("Confusion matrix of simple tree")
table <- knitr::kable( computeConfusionMatrix(simple.tree, data.test ) ) 
print(table)
print("Accuracy, Sensitivity and Specificity of simple tree")
print( computeAccuracySensitivitySpecificity(simple.tree, data.test)  )
 
```

The learning and test datasets are definied as a random sample of the data, therefore they are different every time the program is executed. The model (and its performance) vary accordingly. In order to achieve a stable performance assessment we need to repeat the procedure several times, and return the average result.


```{r, error=FALSE, warning=FALSE, message=FALSE}
compute.Kfold.AccuracySensitivitySpecificity <- function(data,  k_folds=10, minsplit, cp) {
  data_f <- data %>% sample_frac(1) %>% 
  group_by(isTop) %>% 
  mutate(fold=rep(1:k_folds, length.out=n())) %>% ungroup
  
  folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  for(i in 1:k_folds){
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    
    # learn on  data.other.folds
    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = minsplit, cp = cp)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )

    #save results
    folds.results <-  folds.results %>% 
    add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = minsplit)
  }
  
  return(folds.results)
}
```

TODO Now we can use the compute.Kfold.accuracy() function to show how accuracy varies in different tests. Accuracy on learning data is higher and more stable, while accuracy on test shows larger variations.

```{r, error=FALSE, warning=FALSE, message=FALSE}
folds.results = compute.Kfold.AccuracySensitivitySpecificity(data,  k_folds=20, minsplit=20, cp=-1)

p01 <- ggplot(data = folds.results)+ggtitle("k-fold results")+ylab("")+xlab("")+
  geom_line( aes(x=f, y=ac.test,color = "accuracy"))+
  geom_line( aes(x=f, y=sp.test,color = "specificity"))+
  geom_line( aes(x=f, y=se.test,color = "sensitivity"))+ 
  theme(legend.position="top")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(.5,1)
 
p02 <- folds.results %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("summary results")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(.5,1)


wrap_plots(p01,p02,  guides = 'collect')
```
Accuracy and specificity are above the given threshold, but sensitivity is below. The model identifies correctly the negative samples (companies with a low financial rating) but performs poorly on true positive rate i.e. the number of positive samples that are correctly identified as positive.

# Annex 8: Optimization (nested k-fold cross validation)
Imbalance is just one of the potential issues hindering performance: increasing tree complexity, we should achieve better results. A complex tree may be expected to perfectly fit the data (i.e. excellent performance on the learn data set), but an exceessive complexity oftel leads to overfitting (i.e. the model fits the data rather than the system, and performance on unseen data decreases). 

We can spot overfitting by plotting accuracy on test and learn data against a complexity parameter (in our case, minsplit).

```{r, error=FALSE, warning=FALSE, message=FALSE}
tuning.results = tibble() 

for (mspl in seq(1,300,5)){
    tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data.learn,  k_folds=10, minsplit=mspl, cp=-1 ))
}
```

```{r}
summ.results<-tuning.results %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

 p03 <- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = ac.test, group = f, colour = 'accuracy test')) + 
  geom_line(aes(x=minsplit, y = ac.learn,group = f, colour = 'accuracy learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","red")) +  ylim(.7,1)+ ylab("")

p04<- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = se.test, group = f, colour = 'sensitivity test')) + 
  geom_line(aes(x=minsplit, y = se.learn,group = f, colour = 'sensitivity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","green"))+  ylim(.7,1)+ ylab("")

p05<-summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity test')) + 
  geom_line(aes(x=minsplit, y = sp.learn,group = f, colour = 'specificity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","blue"))+  ylim(.7,1)+ ylab("")

wrap_plots(p03,p04,p05, guides = 'collect')
#TODO adjust colors: learning as dotted, colors according to acc, se, sp

```
## comparison of accuracy, specificity and selectivity

```{r}
#specificity VS sensitivity
p06<-summ.results %>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity')) + 
  geom_line(aes(x=minsplit, y = se.test,group = f, colour = 'sensitivity')) + 
  geom_line(aes(x=minsplit, y = ac.test,group = f, colour = 'accuracy')) + 
  #geom_line(aes(x=minsplit, y = sp.se, group = f, colour = 'sp/se')) + 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)
p06
```
At this stage we can identify the value of minsplit that gives optimal results. 
The model does not meet our expectations: accuracy is around .8 for a range of minsplit between 50 and 150, where sensitivity and specificity are unbalanced. 

```{r}
#specificity VS sensitivity
p07<-summ.results %>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test,  colour = 'specificity')) + 
  geom_line(aes(x=minsplit, y = se.test, colour = 'sensitivity')) + 
  geom_line(aes(x=minsplit, y = ac.test, colour = 'accuracy')) + 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)


p08<-summ.results %>%  ggplot() + theme_minimal()+ 
  geom_line(aes(x=minsplit, y = sp.se)) + 
  geom_hline(yintercept = 1,  col = 'black', size=1)+xlab("")+ylab("specificity/sensitivity")


p09<-summ.results %>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = n))+xlab("")+ylab("number of nodes")


design <- "
A
B
C"

wrap_plots(A = p07, B= p08, C=p09, design = design)

```

 ## Optimal tree
 
```{r}
acceptable.results  <- summ.results %>% arrange(desc(se.test)) %>% head(20) 
optimal.minsplit <- acceptable.results %>% head(1) %>% select(minsplit) %>% pull
optimal.tree <- rpart(  isTop~.,data.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit, 
                        cp = -1)
rpart.plot(optimal.tree)

folds.results = compute.Kfold.AccuracySensitivitySpecificity(data,  k_folds=50, minsplit=optimal.minsplit, cp=-1)

p01 <- ggplot(data = folds.results)+ggtitle("k-fold results")+ylab("")+xlab("")+
  geom_line( aes(x=f, y=ac.test,color = "accuracy"))+
  geom_line( aes(x=f, y=sp.test,color = "specificity"))+
  geom_line( aes(x=f, y=se.test,color = "sensitivity"))+ 
  theme(legend.position="top")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)
 
p02 <- folds.results %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("summary results")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)


wrap_plots(p01,p02,  guides = 'collect')
```

optimize and experimental evaluation: we can define a procedure for tuning and testind, using nested kfold cross validation. In this case we select the interval with acceptable sensituvity to specificity ratio and optimize on minsplit

# Auto Tune Tree

```{r, error=FALSE, warning=FALSE, message=FALSE}

AutoTuneTree <- function(data.learn, data.test, k_folds_testing, k_folds_tuning, minsplit_candidates) {
   
    folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

   
  #split data into folds
  data_f <- data %>% sample_frac(1) %>% group_by(isTop) %>% 
    mutate(fold=rep(1:k_folds_testing, length.out=n())) %>% ungroup
 
  #outer cycle: testing
  for(i in 1:k_folds_testing){
    print("starting cycle")
    print(i)
    #select the fold to be used for testing
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    tuning.results = tibble()
    
    # inner cycle: tuning
    for (mspl in minsplit_candidates){
      tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data.learn,  k_folds=10, minsplit=mspl, cp=-1 ))    }
    
    #select optimal value for minsplit
    summarytable  <- tuning.results %>% 
      group_by(minsplit)%>%
      summarise( ac.test = mean(ac.test), minsplit = minsplit) %>% 
      arrange(desc(ac.test))  
    mspl.star <- summarytable %>%  head(1) %>% select(minsplit) %>% pull
    # 
    # print("Mis Split Star")
    # print(mspl.star)
    # print(summarytable)

    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = mspl.star, cp = -1)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )
    #save results
    folds.results <-  folds.results %>%
      add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = mspl.star)
  

  }
  # sort fold results on descending values of ac.test and minsplit
  # fold.results[1] is the highest minsplit that produces the highest accuracy 
  return(folds.results)
}
```


```{r, error=FALSE, warning=FALSE, message=FALSE}
 
#split train and test data 
fraction = .8
indexes.learning = sample(c(1:nrow(data)))[1:(nrow(data)*fraction)]
data.learn <- data %>% slice( indexes.learning) 
data.test  <- data %>% slice(-indexes.learning)

# define the range for minsplit parameter that give high sensitivity
minsplit_candidates = seq(1,240,20)

#define k-fold validation (computational cost vs quality)
k_folds_tuning=3
k_folds_testing=3
optimal.params <- AutoTuneTree(data.learn, data.test, 
                               k_folds_testing, 
                               k_folds_tuning, 
                               minsplit_candidates)

```
final result 
```{r, error=FALSE, warning=FALSE, message=FALSE}
optimal.minsplit  <- optimal.params %>% head(1) %>% select(minsplit) %>% pull
optimal.acc.learn <- optimal.params %>% head(1) %>% select(ac.learn) %>% pull
optimal.acc.test  <- optimal.params %>% head(1) %>% select(ac.test) %>% pull

optimal.tree <- rpart(  isTop~.,data.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit, 
                        cp = .001)
rpart.plot(optimal.tree)

summ.results<-optimal.params %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

p10 <- folds.results %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("summary results for optimal tree")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)
p10
```

 

```{r, error=FALSE, warning=FALSE, message=FALSE}
tibble(features=names(optimal.tree$variable.importance) , imp=optimal.tree$variable.importance) %>%
  mutate(imp = round(imp,1))%>%
  ggplot(aes(reorder(features, imp, sum), imp))  + coord_flip()+
  labs(x="", y="variable importance") +
  geom_bar(stat="identity", fill="light green")+ 
  geom_text(aes(label=imp))
```

# More stuff (to be removed)
```{r}

#optimal minsplit and tree compexity
# p07<-summ.results %>%  ggplot() + theme_minimal()+
#   geom_line(aes(x=minsplit, y = n, group = f, colour = 'number of nodes')) 
# p07

```


[TODO] at this stage we have optimal accuracy and good balance of sp se

We build a procedure based on 2 nested k-fold cross validation

the outer loop is performance assessment
  the inner loop is tuning


Now we can apply the function


## 4.3 Results and discussion

 4. Performance on inbalanced datasts (y8)

Previous results were obtained on a balanced dataset. However, often datasets of interest are inbalances, i.e. the proportion of "true" labeels is a minority. In this project we have two inbalanced datasets: 

- y8 (about 15%)
- y9 (less than 10%)

```{r , error=FALSE, warning=FALSE, message=FALSE}
y8 <- ys %>% mutate(isTop = as.factor(isTop8)) %>%  select(idCompany,isTop)
data8 <- X %>% inner_join(y8) %>% select(-idCompany)
data.learn8 <- data8 %>% slice( indexes.learning) 
data.test8  <- data8 %>% slice(-indexes.learning)

optimal.params8 <- AutoTuneTree(data.learn8, data.test8, k_folds_testing, k_folds_tuning, minsplit_candidates)
optimal.minsplit8 <-  optimal.params8 %>% head(1) %>% select(minsplit) %>% pull
optimal.tree8 <- rpart( isTop~., 
                        data.learn8, 
                        method = "class", 
                        minsplit = optimal.minsplit8, 
                        cp = -1)
rpart.plot(optimal.tree8)
 
# y9 <- ys %>% mutate(isTop = as.factor(isTop9)) %>%  select(idCompany,isTop)
# data9 <- X %>% inner_join(y9) %>% select(-idCompany)
# data.learn9 <- data9 %>% slice( indexes.learning) 
# data.test9  <- data9 %>% slice(-indexes.learning)
# 
# optimal.params9 <- AutoTuneTree(data.learn9, data.test9, k_folds_testing, k_folds_tuning, minsplit_candidates)
# optimal.minsplit9 <-  optimal.params9 %>% head(1) %>% select(minsplit) %>% pull
# optimal.tree9<- rpart( isTop~., 
#                         data.learn9, 
#                         method = "class", 
#                         minsplit = optimal.minsplit9, 
#                         cp = -1)
# rpart.plot(optimal.tree9)


```






```{r}
folds.results8 <- compute.Kfold.AccuracySensitivitySpecificity(data.test8,  
                                                               k_folds=10, 
                                                               minsplit=optimal.minsplit8, 
                                                               cp= -1)
# folds.results9 <- compute.Kfold.AccuracySensitivitySpecificity(data.test9,  
#                                                                k_folds=10, 
#                                                                minsplit=optimal.minsplit9, 
#                                                                cp= -1)
```

```{r, echo=FALSE}
plot8 <- folds.results8 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("y8 (imbalanced)")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0,1)

# plot9 <- folds.results9 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
#   ggplot(aes(x=name, y=value, color = name))+
#   geom_boxplot(show.legend = FALSE)+ggtitle("y9 (imbalanced)")+ylab("")+xlab("")+
#   geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0,1)

#wrap_plots(plot8,plot9, guides = 'collect') 
```
this is only apparently better - actually is misleading since sensitivity drops. We are facing "accuracy paradox". Take for example the strongly unbalanced case of y9: in this case the label y9 is false in 94% of case, so we can achieve an overall accuracy of 94% just predicting always false regardless of input data. In this case the relevant parameter to assess quality of prediction is sensitivity. 

We can improve the performance by
- oversampling minority class
- using random forrest algorithm

```{r}
library(caret)
library(DMwR2) # for smote implementation
library(pROC)  # for AUC (area under the curve) 
set.seed(123)

# Create model weights (they sum to one)
data8r <- data8 %>% 
  mutate(Class = case_when(isTop== TRUE   ~ "top" , isTop== FALSE   ~ "other")) %>% 
  select(-isTop)  

imbal_train <- data8r %>% slice( indexes.learning)
imbal_test  <- data8r %>% slice(-indexes.learning) 
prop.table(table(imbal_train$Class))

# Build custom AUC function to extract AUC from the caret model object
test_roc <- function(model, data) {
  roc(data$Class, predict(model, data, type = "prob")[, "top"])
}

# Set up control function for training
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,#10
                     repeats = 3,#5
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Build a random forrest as original fit
selected.method = "rf"
orig_fit <- train(Class ~ .,
                  data = imbal_train,
                  method = selected.method,
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl)
orig_fit %>% test_roc(data = imbal_test) %>%  auc()
  
 
#Create model weights (they sum to one)
model_weights <- ifelse(imbal_train$Class == "top",
                        (1/table(imbal_train$Class)[1]) * 0.9,
                        (1/table(imbal_train$Class)[2]) * 0.1)
ctrl$seeds <- orig_fit$control$seeds
weighted_fit <- train(Class ~ .,
                      data = imbal_train,
                      method = selected.method,
                      verbose = FALSE,
                      weights = model_weights,
                      metric = "ROC",
                      trControl = ctrl)

# Build down-sampled model
ctrl$sampling <- "down"
down_fit <- train(Class ~ .,
                  data = imbal_train,
                  method = selected.method,
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl)
# Build up-sampled model
ctrl$sampling <- "up"
up_fit <- train(Class ~ .,
                data = imbal_train,
                method = selected.method,
                verbose = FALSE,
                metric = "ROC",
                trControl = ctrl)
# Build smote model
ctrl$sampling <- "smote"
smote_fit <- train(Class ~ .,
                   data = imbal_train,
                   method = selected.method,
                   verbose = FALSE,
                   metric = "ROC",
                   trControl = ctrl)

# Examine results for test set
model_list <- list(original = orig_fit,
                   #weighted = weighted_fit,
                  # down = down_fit,
                   up = up_fit)
                   #SMOTE = smote_fit
                   
model_list_roc <-  model_list %>%  map(test_roc, data = imbal_test)

model_list_roc %>% 
  map(auc)

model_list_roc

results_list_roc <- list(NA)
num_mod <- 1
for(the_roc in model_list_roc){
  
  results_list_roc[[num_mod]] <- 
    data_frame(tpr = the_roc$sensitivities,
               fpr = 1 - the_roc$specificities,
               model = names(model_list)[num_mod])
  
  num_mod <- num_mod + 1
  
}
results_df_roc <- bind_rows(results_list_roc)
# Plot ROC curve for all 5 models
custom_col <- c("black", "red", "yellow")
ggplot(aes(x = fpr,  y = tpr, group = model), data = results_df_roc) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = custom_col) +
  geom_abline(intercept = 0, slope = 1, color = "gray", size = 1) +
  theme_bw(base_size = 18)



```

```{r}

#now we can check the rpart "optimal tree"
predicted.y = predict(optimal.tree8, data.test, type="class")
cm.optimal.tree <- confusionMatrix( predicted.y, as.factor(data.test$isTop) )

#now we can check original model
predicted.y = predict(orig_fit, imbal_test)
cm.rf.imbalanced <- confusionMatrix( predicted.y, as.factor(imbal_test$Class))

#now we can test up_fit
predicted.y = predict(up_fit, imbal_test)
cm.rf.upfit <- confusionMatrix( predicted.y, as.factor(imbal_test$Class))

data.to.plot <-  tibble('indic'='', 'method'='', 'value'=0)%>% head(0)
data.to.plot  <- data.to.plot %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  '1 regression tree', value =  cm.optimal.tree$overall[1])) %>%
  add_row( tibble_row(indic = 'accuracy',   method =  '2 random forrest',   value =  cm.rf.imbalanced$overall[1])) %>%
  add_row( tibble_row(indic = 'accuracy',   method =  '3 oversampling + rf',value =  cm.rf.upfit$overall[1])) %>%
  add_row( tibble_row(indic = 'sensitivity',method =  '1 regression tree', value =  cm.optimal.tree$byClass[1])) %>%
  add_row( tibble_row(indic = 'sensitivity',method =  '2 random forrest',   value =  cm.rf.upfit$byClass[1])) %>%
  add_row( tibble_row(indic = 'sensitivity',method =  '3 oversampling + rf',value =  cm.optimal.tree$byClass[1])) %>%
  add_row( tibble_row(indic = 'specificity',method =  '1 regression tree', value =  cm.optimal.tree$byClass[2])) %>%
  add_row( tibble_row(indic = 'specificity',method =  '2 random forrest',   value =  cm.rf.imbalanced$byClass[2])) %>%
  add_row( tibble_row(indic = 'specificity',method =  '3 oversampling + rf',value =  cm.rf.upfit$byClass[2]))  

data.to.plot %>% ggplot()+ geom_line(aes(x = method, y = value, group = indic, color = indic))+
  geom_point(aes(x = method, y = value, shape = method, color = indic, size = 4))+ ylim(0,1)+ 
  geom_hline(yintercept=.8,linetype = 'dotted', col = 'black', size=1)+ xlab("")
 
```


The figure above shows that the quality of prediction on inbalanced datasets can be improved using suitable techiques, in this case a *random forrest* model and an oversampling of the minority class. 




<!-- ```{r, echo=FALSE} -->

<!-- proportions %>% pivot_longer(-c(name), names_to = "type", values_to = "value") %>% -->
<!--   ggplot(aes(x = name, y =value, fill=type))+ -->
<!--   geom_bar(, stat = "identity", position = position_fill(reverse = TRUE)) + -->
<!--   scale_fill_manual("legend", values = c( "minority.class" = "light green","majority.class" = "light blue")) + -->
<!--   ylab("") + xlab("")+scale_y_continuous(labels = scales::percent) -->
<!-- ``` -->

\newpage
\newpage
\newpage
\newpage
 
