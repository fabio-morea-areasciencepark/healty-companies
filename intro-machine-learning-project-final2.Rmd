--- 
title: 'Machine Learning project: predicting financial rating of companies'
author: "Fabio Morea"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_width: 6
    fig_height: 3
bibliography: references.bib
link-citations: yes
csl: plos.csl
nocite: '@*'
---

> **Abstract**: This notebook describes the final project for the course *introduction to machine learning* and, at the same time, a case study for Area Science Park in the frame of Innovation Intelligence FVG project. The user needs to predict, with a machine learning model, if a company has a high financial rating. The available data are the financial statements of the companies and, for a limited number of them, the financial rating that will be used as a label for the training of the model. The problem is to identify the model and the data preparation strategy that ensures the best performance on two different datasets, a balanced one (y7, minority class 45%) and the unbalanced one (y8, minority class 6%). 
In the case of the *balanced dataset*, the best performances are obtained with a Random Forrest model, but also a regression tree leads to good performances associated with an excellent interpretability and explainability of the results.In the case of *inbalanced dataset* it is necessary to apply an oversampling technique on the learning dataset which, associated with a random forrest, gives results in line with those of the previous case.


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#libraries
require(tidyverse)
require(ggthemes)
require(patchwork)
require(ggcorrplot)  
require(scales)

require(rpart)
require(rpart.plot)

require(caret)
require(pROC)  # for AUC (area under the curve) 

#default values
theme_set( theme_hc(base_size = 12))
set.seed(123)
```

```{r functions, include=FALSE}
# captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
``` 
 

# 1. Introduction and problem statement

## Background information

Research, innovation and highly skilled people are considered to be important factors in economic and social development. Economic support policies often include funds to support research (for example with the creation of public research infrastructures), companies (for example with tenders to co-finance innovative projects) and the training of people with the necessary skills.

Area Science Park [@AREA] is a national research institution that manages a science and technology park located in Trieste (Italy) and is engaged in several projects aiming to support innovation at the regional level. Innovation Intelligence FVG [@iifvg] is a project, managed by Area Science Park and supported by Regione Friuli Venezia Giulia, which aims to monitor the performance of companies in terms of economic, employment and innovation results. .

In the frame of Innovation Intelligence project, Area Science Park needs to analyse the performance of groups of companies (such as the *tenants of Area Science Park*, about 60 companies that have their premises or research laboratories in the science and technology park, or the *regional cluster of metal and plastic manufacturing*, identified by a list of NACE activity sectors [@Eurostat]) comparing them against similar groups in other regions. Information on companies is available in several datasets that either open source or available under a license for the project partners. A relevant source of information, **financial rating** is available from a rating agency [@rating], only for the companies that have their premises in Friuli Venezia Giulia. 

Financial rating is a numerical variable ranging from 1 to 10, where low values denote an insufficient capability to meet financial obligations, and high values denote very good or excellent reliability. The value is generated by a proprietary algorithm summarizing the overall performance of a company in all its economic and financial areas: profitability, liquidity, solvency, efficiency, production. 
Only a part of the data used to generate financial ratings is available for this project, thus we may expect that the actual value will be hard to predict. According to some similar cases described in literature [@Gandin2019Can],[@Wu2021Credit] reasonably good predictions of rating can be based on *balance sheet data*. In our case we can access additional information on employees (number of employees, turnover and net balance in a given year), that may be a good proxy for company health and performance.

## Objective and constraints

The Innovation Intelligence team at Area Science Park wants to explore the potential use of Machine Learning techniques to improve to predict the financial rating of companies, based on other relevant datasets (general company information from the *Italian Business Register* [@IBR].

> The **objective** of the project to identify the model that ensures the best performance on two different cases, a balanced datasets, and an imbalanced datasets (which is notoriously a harder challenge for machine learning algorithms).

The user - or **target audience** - of the project study is the Innovation Intelligence team at Area Science Park, a small group of economic analysts, that have a robust domain knowledge but limited experience in data science. The **datasets** are given: companies working in the sector of metal and plastic manufacturing in Friuli Venezia Giulia, identified by NACE sectors 23 to 29. 

Since the results of the project will be used by the Innovation Intelligence team to predict financial ratings of other groups of companies, some **constraints** apply. The analysis will be restricted to a binary classification, in order to obtain a relatively simple but still significant preliminary result. Explainability and interpretability of the model are appreciated, but not essential.  Training and classification will be generally performed on a laptop, once a month, on datasets composed of 200 to 2000 companies, hence there are no specific constraints on time or computation effort. 



## Formal statement of the problem

Let a company be represented by a vector *X* in a multidimensional space, and associated with a binary label *y* indicates whether the company belongs to a group of *top performers* or not. The objective is to assess the performance of a binary classification model that predicts *y* under the following conditions: the dataset is  composed of at least 1000 observations, homogeneous by sector and and company type, and the computation time musl be less than 1 hour on a state-of-the art personal computer.

The performance shall be assessed in two cases: a balanced dataset in which the minority class is between 40% and 50%, and an inbalanced dataset in which the minority class is less than 10%.

# 2. Assessment and performance indexes

Three indexes will be used to measure performance of each model, and effectiveness on unseen data: accuracy, sensitivity (or True Positive Rate, TPR, denotes the ability to correctly classify companies that are in the "top" group) and specificity (or True Negative Rate, TRN, denotes the ability to correctly classify companies that are in the "other" group). 

$$Accuracy = (True Positive +True Negative)/AllCases$$
$$Sensitivity = True Positive / (True Positive + False Negative) = 1 - FNR$$ 
$$Specificity = True Negative/ (True Negative + False Positive) = 1- FPR$$
The performance will be assessed experimentally using a k-fold cross validation procedure.

As the objective is to identify which model achieves the best performance, there is no predetermined threshold for the accuracy, specificity and sensitivity. However, according to literature and previous experience, accuracy, sensitivity and specificity may be expected to be of the order of 80%. Such level may seem relatively high compared if compared to safety-critical applications, but is sufficient for the business case and in line with similar cases of binary classification of company performance found in scientific literature. 

# 3. Proposed solution

The proposed solution is to evaluate the performance of two binary classification models: 

- a *binary decision tree*, that offers the advantage of explainability and interpretability 
- a *random Forrest*, that offers potentially a higher performance, according to a comprehensive literature eview [@fernandez2014we]. 

on two different datasets, *Top7* (balanced) and *Top8*(unbalanced). The key steps to achieve the proposed solution are:

1) **feature engineering**: select and rescale features to be processed (matrix X) and generate the labels to be predicted for the two datasets (y7 for Top7 and y8 for Top8).

2) **assessment on the balanced dataset**, using the rpart() library, explore the effect of a flexibility parameter (minsplit) on the performance indexes (accuracy, sensitivity and selectivity). Identify an appropriate range for the flexibility parameter and optimize the decision tree, using a nested k-fold cross validation procedure. Finally, fit a Random Forrest model using caret library and compare the results.

3) **assessment on the inbalanced dataset**: using the rpart() library, explore the effect of a flexibility parameter (minsplit) on the performance indexes (accuracy, sensitivity and selectivity), as seen above. to produce a training dataset that Identify an appropriate range for the flexibility parameter and optimize the decision tree, using a nested k-fold cross validation procedure. Apply oversampling techniques (Up-sampling and down-sampling) to be used for training a regression tree and a random Forrest. 

# 4. Experimental evaluation

## 4.1 Data

The data available from Innovation Intelligence needs to be pre-processed in order to obtain a *tidy* dataset suitable for ML. An extended explanation of all the features available in the original dataset is available in the Annexes.

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
require(tidyverse)
pathTidyData = './../../_data/tidy/'
X <- read_csv(paste0(pathTidyData,"X.csv")) %>% mutate(idCompany = as.factor(idCompany))
ys <- read_csv(paste0(pathTidyData,"y.csv")) %>% mutate(idCompany = as.factor(idCompany))
```

The original dataset $X$ consists of n = `r nrow(X)` observations and p = `r ncol(X)` features (namely: `r names(X)` ). 
The sample is selected according to NACE codes and company type. The first selection on **company type** : we select all types that have a duty of disclosure of financial information, and therefore are suitable for the analysis, namely SU (società a responsabilità limitata con unico socio), SR (società a responsabilità limitata), SP (società per azioni), SD (società europea),  RS (società a responsabilità limitata semplificata), RR (società a responsabilità limitata a capitale ridotto), AU (società  per azioni con socio unico), AA (società in accomandita per azioni.

A further selection is based on **NACE codes** (further information in the Annexes).The selected sample is composed of companies that have at least one NACE code in one of the following Divisions: 22 (Manufacture of rubber and plastic products), 23 (Manufacture of other non-metallic mineral products), 24 (Manufacture of basic metals), 25 (Manufacture of fabricated metal products, except machinery and equipment), 26 (Manufacture of computer, electronic and optical products), 27 (Manufacture of electrical equipment) and 28 (Manufacture of machinery and equipment).

Some filters are applied: time-dependent data (such as balance sheet data, rating and employees flows) are filtered to year 2019, and company age (years in business) is filtered to values greater than 1.

Labels y7 and y8 are generated on the basis of financial ratings. Data is  available for years 2018, 2019 and 2020; for the purpose of this study a single year (2019) will be selected.   The choice of classes highlights a relevant issue in classification: imbalance in the distribution of labels.The balanced dataset isTop7 groups basically all above-average companies, while isTop8 groups about one quarter of the companies.  In the following, the underrepresented class will be referred to as *minority class*, and the over represented class is referred to as *majority class*.

```{r echo=FALSE, error=FALSE, fig.cap=paste("majority and minority classes in dataset Top7 and Top8"), fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
proportions <- ys %>% pivot_longer(cols=c( isTop8, isTop7)) %>%
  group_by(name) %>%
  summarise(minority.class = round(mean(value),3)) %>%
  mutate(majority.class = 1 - minority.class)
knitr::kable(proportions)

```


The objective of feature engineering is to build a dataset that contains a set of relevant variables for learning and prediction, appropriately scaled, in the form of a matrix. Specifically we will focus on calculating new features based on domain knowledge, checking variable correlation and normalizing the selected features by centering and scaling. 
The original features are highly correlated, as highlighted in the following correlation matrix.  

Moreover, feature values range over different orders of magnitude (company age ranges from 1 to 150, while total assets ranges from 0 to $10^9$ €).

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
X <- X %>% select(idCompany, totAssets,totEquity, noi,personnel,prod, debts,deprec,valCost,totIntang, revenues,valAdded,yearsInBusiness, staffTurnover, staffBalance, StockAll) %>% na.omit()
#correlation matrix
cm <- X %>% select(where(is.numeric), -idCompany) %>% cor() 
p.fe.1 <-ggcorrplot(cm,  type = "upper",insig = "blank") +
  theme(axis.text.x=element_text(size=6, angle=90))+
  theme(axis.text.y=element_text(size=6, angle=0))+
  labs(x="", y="") 
  
#boxplot
p.fe.2 <- ggplot(stack(X), aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="") +
 geom_boxplot(fill = "white", colour = "black") + coord_flip() +
  theme(axis.text.x=element_text(size=12, angle=0))+
  theme(axis.text.y=element_text(size=6, angle=0))+
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
              labels = trans_format("log10", math_format(10^.x)))+ 
  #annotation_logticks(sides='b')+
  theme(axis.text.x = element_text(vjust = -2))+ theme_excel_new() 
```


```{r echo=FALSE, error=FALSE, fig.cap=paste("Correlation and order of magnitude of original features"), fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
wrap_plots(p.fe.1, p.fe.2)
```


We can tackle both issues by calculating new features that scale economic values to the company size, a common practice in economic analysts, that allows direct comparison of company performance regardless of company size. The new features are named 'rel*' as they are scaled to the total assets (totAssets) or the total number of employees (StockAll) of each company.


```{r echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
X <- X %>% 
    mutate(relInt =   totIntang/totAssets*5) %>%
    mutate(relEquity = totEquity/totAssets) %>%
    mutate(relreve   = revenues/totAssets) %>%
    mutate(relNoi    = noi/totAssets) %>%
    mutate(relPers   = personnel/totAssets) %>%
    mutate(relDeprec = deprec/totAssets*5) %>%
    mutate(yearsInBusiness = yearsInBusiness)  %>%
    mutate(relStaffTurnover = staffTurnover/StockAll) %>%
    mutate(relStaffBalance  = staffBalance/StockAll) %>%
    mutate(VAS  = valAdded/StockAll) %>%
    mutate(logStaff  = log10(StockAll)) %>%
    select(-totAssets,-totEquity, -noi,-personnel,-debts,-prod,
          -deprec,-revenues,-valCost,-totIntang,-valAdded,
          -StockAll, -staffTurnover, -staffBalance)%>% na.omit()
```
Correlation between the new features has significantly improved, as shown in the correlation matrix below.  

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
cm1 <- X %>% select(where(is.numeric)) %>% cor() 
p.fe.3 <-ggcorrplot(cm1,   type = "upper", insig = "blank") +
  theme(axis.text.x=element_text(size=6, angle=90))+
  theme(axis.text.y=element_text(size=6, angle=0))
```

The next step is to normalize (center and rescale) numeric features to the a similar range in order to improve the performance of the learning algorithm. 

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
X <- X %>% mutate(across(is.numeric, ~ as.numeric(scale(.)))) 
p.fe.4 <- X %>%  select(-idCompany)  %>% stack() %>% ggplot(aes(x = ind, y = values)) +
 stat_boxplot(geom = "errorbar", width = 0.5) +
 labs(x="", y="normalized features") +
 geom_boxplot(fill = "light gray", colour = "black") + coord_flip()  +  scale_y_continuous(  limit = c(-3, 3)) + geom_hline(yintercept=0, color = "red") +
  theme(axis.text.x=element_text(size=6, angle=0))+
  theme(axis.text.y=element_text(size=6, angle=0))

```

```{r echo=FALSE, fig.cap=paste("Correlation and distribution of features, after features engineering"), fig.height=4, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
wrap_plots(p.fe.3,p.fe.4)
```

All features are of the same order of magnitude. The dataset is composed of n = `r nrow(data)` observations and p = `r ncol(data)` features (namely: `r names(data)` ). There meaning of variables is  self explanatory in some cases, but economic features may require an explanation:  


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# this procedure is part of the Annexes
data <- X %>% inner_join(ys) %>% select(-idCompany)

y7 <- ys %>% select(idCompany,isTop7)  %>% 
  mutate(isTop = as.factor(isTop7)) %>% 
  select(-isTop7)%>%
  mutate(idCompany = as.factor(idCompany)) 
data7 <- X %>% inner_join(y7) %>% select(-idCompany)


y8 <- ys %>% select(idCompany,isTop8)  %>% 
  mutate(isTop = as.factor(isTop8)) %>% 
  select(-isTop8)%>%
  mutate(idCompany = as.factor(idCompany))
data8 <- X %>% inner_join(y8) %>% select(-idCompany)

p.learn = .8
indexes.learning = sample(c(1:nrow(data)))[1:(nrow(data)*p.learn)]
data7.learn = data7[ indexes.learning,]
data7.test =  data7[-indexes.learning,]
data8.learn = data8[ indexes.learning,]
data8.test =  data8[-indexes.learning,]
```

## 4.2 Performance of decision trees on balanced dataset 

Using the rpart() library, we can explore the effect of a flexibility parameter (minsplit) on the performance indexes (accuracy, sensitivity and selectivity). The decision tree will be tuned using the parameter minsplit, and setting the parameter cp to a fixed value (cp = .001) in order to allow a complexity budget, and keep the model reasonably simple to enhance explainability and interpretability. 

 
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
cp_fixed = .0001
```

The first step is to identify an appropriate range for the flexibility parameter and optimize the decision tree, using a nested k-fold cross validation procedure. Assessment of binary decision trees will be performed on a customized function *assessAccuracySensitivitySpecificity()*, while the random Forrest will be assessed using *caret* package. 

The quality of prediction of a single decision tree can be assessed computing its **confusion matrix** (a 2x2 table in the case of binary classifiers) that shows the number of values classified correctly (on the main diagonal) or misclassified (off-diagonal). Error rate, accuracy, specificity and sensitivity are computed from the confusion matrix, comparing model predictions with known labels on *unseen data* from the test dataset.

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# this procedure is part of the Annexes
computeConfusionMatrix = function(model, data) {
  predicted.y = predict(model, data, type="class")
  return(table(predicted.y, data$isTop))
}
```


 
```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# this procedure is part of the Annexes
computeAccuracySensitivitySpecificity =  function(model, data) {
  conf.matrix <- computeConfusionMatrix (model, data)
  true.neg  <- conf.matrix[2,2]
  false.neg <- conf.matrix[1,2]
  false.pos <- conf.matrix[2,1]
  true.pos  <- conf.matrix[1,1]
  pos = true.pos + false.neg
  neg = true.neg + false.pos
  accuracy <- (true.pos + true.neg) / (pos+neg)
  sensitivity <- true.pos/(true.pos+false.neg) # Sensitivity: The “true positive rate” 
  specificity <- true.neg/(true.neg+false.pos)# Specificity: The “true negative rate”
  return( c(accuracy, sensitivity, specificity) )
}
```

Learning and prediction are based a stochastic process (splitting data into learn and test sets), hence the results vary for each sample. In order to evaluate the average performance of the method, we need to assess it over a number of samples using a structured experimental procedure such as k-fold cross validation, using the custom function compute.Kfold.AccuracySensitivitySpecificity() 


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
compute.Kfold.AccuracySensitivitySpecificity <- function(data,  k_folds=5, minsplit, cp) {
  data_f <- data %>% sample_frac(1) %>% 
  group_by(isTop) %>% 
  mutate(fold=rep(1:k_folds, length.out=n())) %>% ungroup
  
  folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  for(i in 1:k_folds){
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    
    # learn on  data.other.folds
    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = minsplit, cp = cp)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )

    #save results
    folds.results <-  folds.results %>% 
    add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = minsplit)
  }
  
  return(folds.results)
}
```

Using the rpart() library, explore the effect of a flexibility parameter (minsplit) on the performance indexes (accuracy, sensitivity and selectivity). We scan the assessment indexes over a wide range of minsplit, from 1 to 300 (step 5). 


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
tuning.results = tibble() 

for (mspl in seq(1,300,5)){
    tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data7.learn,  k_folds=5, minsplit=mspl, cp=cp_fixed ))
}
```

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
summ.results<-tuning.results %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

 p03 <- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = ac.test, group = f, colour = 'accuracy test')) + 
  geom_line(aes(x=minsplit, y = ac.learn,group = f, colour = 'accuracy learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","red")) +  ylim(.7,1)+ ylab("accuracy")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))

p04<- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = se.test, group = f, colour = 'sensitivity test')) + 
  geom_line(aes(x=minsplit, y = se.learn,group = f, colour = 'sensitivity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","green"))+  ylim(.7,1)+ ylab("sensitivity")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))

p05<-summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity test')) + 
  geom_line(aes(x=minsplit, y = sp.learn,group = f, colour = 'specificity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","blue"))+  ylim(.7,1)+ ylab("specificity")+ xlab("minsplit")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  guides(color = guide_legend(title = ""))
  


```

```{r eval=FALSE, include=FALSE}
# mm<-readRDS('model_tree_8.rds')
# conf.matrix<-computeConfusionMatrix(mm, data8.learn)
#   true.neg  <- conf.matrix[2,2]
#   false.neg <- conf.matrix[1,2]
#   false.pos <- conf.matrix[2,1]
#   true.pos  <- conf.matrix[1,1]
#   pos = true.pos + false.neg
#   neg = true.neg + false.pos
#   accuracy <- (true.pos + true.neg) / (pos+neg)
#   sensitivity <- true.pos/(true.pos+false.neg) # Sensitivity: The “true positive rate” 
#   specificity <- true.neg/(true.neg+false.pos)# Specificity: The “true negative rate”

```



```{r echo=FALSE, error=FALSE, fig.cap=paste("Experimental evaluation of accuracy, sensitivity and selectivity for a single decision tree on the balanced dataset"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
wrap_plots(p03,p04,p05, guides = 'collect')
```

The figure below shows the results, comparing results on training data (black lines) against results on test data (colored lines). The horizontal axis is minsplit, therefore low values refer to complex trees, that have very high values of accuracy, sensitivity and specificity. Accuracy has several maxima for minsplit in range 80-200, and drops for smaller value (a typical sign of overfitting). Sensitivity and specificity drop for low values of minsplit, but have opposite behavior for intermediate values. 
Sensitivity is the percentage of true positives out of all observations that are in the Top group, i.e. it is the ability of the model to correctly classify a company that is a top performer.
Symmetrically, specificity is the percentage of true negatives out of all observations that are not in the Top group, i.e. it is the ability of the model to correctly classify a company that is not top performer.
In this example, a model with minsplit around 150 has a low sensitivity and a high specificity, hence they misclassify companies that are in the top group, and correctly classify companies that are not.

Sensitivity and specificity are inversely related should be considered together, and the choice for optimization depends on the needs of the project. In our example, if we need to optimize the model to achieve a high specificity, we should choose minsplit in the range 80 to 120. conversely, higher sensitivity can be achieved using larger values of minsplit, in the range 200-300.

 
```{r AutoTuneTree, error=FALSE, message=FALSE, warning=FALSE, include=FALSE}

AutoTuneTree <- function(data.learn, data.test, k_folds_testing, k_folds_tuning, minsplit_candidates) {
   
    folds.results = tibble(f = NA, ac.learn = NA, sp.learn = NA, se.learn = NA, 
                                 ac.test = NA,  sp.test = NA,  se.test = NA, 
                                 n = NA, minsplit = NA) %>% head(0)

  #split data into folds
  data_f <- data.learn %>% sample_frac(1) %>% group_by(isTop) %>% 
    mutate(fold=rep(1:k_folds_testing, length.out=n())) %>% ungroup
 
  #outer cycle: testing
  for(i in 1:k_folds_testing){
    
    #select the fold to be used for testing
    data.kth.fold <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    tuning.results = tibble()
    
    # inner cycle: tuning
    for (mspl in minsplit_candidates){
      tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data.learn,  k_folds=10, minsplit=mspl, cp=cp_fixed ))    }
    
    #select optimal value for minsplit
    summarytable  <- tuning.results %>% 
      group_by(minsplit)%>%
      summarise( ac.test = mean(ac.test), minsplit = minsplit) %>% 
      arrange(desc(ac.test))  
    mspl.star <- summarytable %>%  head(1) %>% select(minsplit) %>% pull

    model <- rpart(  isTop~.,data.other.folds,method = "class", minsplit = mspl.star, cp = -1)
    rl <- computeAccuracySensitivitySpecificity(model, data.other.folds )
    rt <- computeAccuracySensitivitySpecificity(model, data.kth.fold   )
    #save results
    folds.results <-  folds.results %>%
      add_row(f = i , 
            ac.learn = rl[1], 
            sp.learn = rl[2], 
            se.learn = rl[3], 
            ac.test  = rt[1],  
            sp.test  = rt[2],  
            se.test  = rt[3],
            n = nrow(model$frame), 
            minsplit = mspl.star)
    }
  # sort fold results on descending values of ac.test and minsplit
  # fold.results[1] is the highest minsplit that produces the highest accuracy 
  return(folds.results)
}
```

Train for max accuracy, in a range of minsplit with high specificity

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
optimal.spec.7 <- AutoTuneTree(data7.learn, data7.test, 
                               k_folds_testing=3, 
                               k_folds_tuning=3, 
                               minsplit_candidates=seq(250,400,2))

```

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}

optimal.minsplit.spec.7  <- optimal.spec.7 %>% head(1) %>% select(minsplit) %>% pull
modelTree7Spec <- rpart(  isTop~.,data7.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.spec.7, 
                        cp = cp_fixed)
 
saveRDS(modelTree7Spec, "model_tree_7_spec.rds")

```



```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}


summ.results.7 <-optimal.spec.7 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

data.to.plot.7 <-  tibble('indic'='', 'method'='', 'value'=0)%>% head(0)#empty tibble 
data.to.plot.7  <- data.to.plot.7 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y7 Tree SPEC', value =  mean(summ.results.7$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y7 Tree SPEC', value =  mean(summ.results.7$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y7 Tree SPEC', value =  mean(summ.results.7$sp.test)))

p10 <- summ.results.7 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  ylim(.7,1)


```

Train another tree for max accuracy, in a range of minsplit with high sensitivity


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
 optimal.sens.7 <- AutoTuneTree(data7.learn, data7.test, 
                               k_folds_testing=3, 
                               k_folds_tuning=3, 
                               minsplit_candidates=seq(50,120,2))



optimal.minsplit.sens.7  <- optimal.sens.7 %>% head(1) %>% select(minsplit) %>% pull
 
summ.results.7 <-optimal.sens.7 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

modelTree7Sens <- rpart(  isTop~.,data7.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.sens.7, 
                        cp = cp_fixed)

saveRDS(modelTree7Sens, "model_tree_7_sens.rds")
#load the model using  <- readRDS("model_tree_7_sens.rds")

data.to.plot.7  <- data.to.plot.7 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y7 Tree SENS', value =  mean(summ.results.7$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y7 Tree SENS', value =  mean(summ.results.7$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y7 Tree SENS', value =  mean(summ.results.7$sp.test)))


```


```{r echo=FALSE, fig.cap=paste("A visual representation of two binary decision trees, optimized in different ranges of the flexibility parameter - balanced dataset y7"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
figures <- par(mfrow=c(1,2)) # more trees in the same figure
modelTree7Spec  %>% rpart.plot(  main="high specificity classification tree", shadow.col = "gray")
modelTree7Sens %>% rpart.plot(  main="high sensitivity classification tree")
par(figures)
```


## 4.3 Performance of random forrest on balanced datast 
A Random Forrest model can be fit to the train data and assessed on the test data, using caret library and the embedded repeated cross validation method. Performance is assessed using the custom function kFoldassesAccuracySensitivityScpecificityRF(). 

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}

kFoldassesAccuracySensitivityScpecificityRF <- function(data, ctrl, name, k_folds=10) {

  data_f <- data %>% 
    sample_frac(1) %>% 
    group_by(Class) %>% 
    mutate(fold=rep(1:k_folds, length.out=n())) %>% 
    ungroup
  
  folds.results = tibble(f = NA, name = '', accuracy = NA,  specificity = NA,  sensitivity = NA) %>% head(0)

  for(i in 1:k_folds){
    data.kth.fold    <- data_f %>% filter(fold==i) 
    data.other.folds <- data_f %>% filter(fold!=i)
    modelRF <- train(Class ~ .,
                    data = data.other.folds,
                    positive = "top",
                    method = "rf",
                    verbose = FALSE,
                    metric = "ROC",
                    trControl = ctrl)
    predicted.y = predict(modelRF, data.kth.fold, type = "prob")[,"top"]
    
    conf.matrix <- table(predicted.y>=.5, data.kth.fold$Class=="top" )
    
  true.neg  <- conf.matrix[1,1]
  false.neg <- conf.matrix[1,2]
  false.pos <- conf.matrix[2,1]
  true.pos  <- conf.matrix[2,2]
    pos = true.pos + false.neg
    neg = true.neg + false.pos
    accuracy <- (true.pos + true.neg) / (pos+neg)
    sensitivity <- true.pos/(true.pos+false.neg) # Sensitivity: The “true positive rate” 
    specificity <- true.neg/(true.neg+false.pos)# Specificity: The “true negative rate”
     
    folds.results <-  folds.results %>% 
    add_row(f = i , name = name,
            accuracy  = accuracy,  
            specificity  = specificity,  
            sensitivity  = sensitivity)
  }
  
  return(folds.results)
}

```


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#generate data do plotrequire(caret)
data7r <- data7 %>% 
  mutate(Class = case_when(isTop== TRUE   ~ "top" , isTop== FALSE   ~ "other")) %>% 
  select(-isTop)  
train.7 <- data7r %>% slice( indexes.learning)
test.7  <- data7r %>% slice(-indexes.learning) 

# general control function for training
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,#10
                     repeats = 3,#5
                     summaryFunction = twoClassSummary,
                     savePredictions = "final",
                     classProbs = TRUE)


data.to.plot.rf.7 <- kFoldassesAccuracySensitivityScpecificityRF(data7r, 
                                                            ctrl,
                                                            name='y7 RF', 
                                                            k_folds=10) 
 
data.to.plot.7  <- data.to.plot.7 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y7 RF', value =  mean(data.to.plot.rf.7$accuracy))) %>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y7 RF', value =  mean(data.to.plot.rf.7$sensitivity))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y7 RF', value =  mean(data.to.plot.rf.7$specificity)))


```


```{r fig.height=3, fig.width=8, include=FALSE}
p01 <- ggplot(data = data.to.plot.rf.7)+ggtitle("RF (k-fold results)")+ylab("")+xlab("")+
  geom_point( aes(x=f, y=accuracy,color = "accuracy"))+
  geom_point( aes(x=f, y=specificity,color = "specificity"))+
  geom_point( aes(x=f, y=sensitivity,color = "sensitivity"))+ 
  theme(legend.position="top")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(.6,1)+
  scale_x_continuous(breaks = seq(1, 10, by = 1))+
   theme(panel.grid.major.x = element_line(color = "gray",
                                        size = 0.5,
                                        linetype = 1)) 
 
p02 <- data.to.plot.rf.7 %>% select(-name, -f)%>% 
  pivot_longer(everything(),names_to = "name" , values_to = "value" )  %>%
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("RF performance indexes")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(.6,1)

```

```{r echo=FALSE, fig.cap=paste("Experimantal assessment of accuracy, sensitivity and specificity for a random forrest model on a balanced dataset y7, using k-fold cross validation"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}

wrap_plots(p01,p02,  guides = 'collect')


```
 

```{r echo=FALSE, error=FALSE, fig.cap=paste("Comparison of results of experimantal assessment of accuracy, sensitivity and specificity for a random forrest model on a balanced dataset y7, using k-fold cross validation"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
data.to.plot.7 %>% 
  mutate(method1 = factor(method, 
                          levels = c('y7 tree SPEC', 'y7 tree SENS',  'y7 RF'  ))) %>%
  ggplot()+ 
  #geom_line( aes(x = method, y = value, group = indic, color = indic))+
  geom_point(aes(x = method, y = value, shape = method, color = indic, size = 4))+
  geom_hline(yintercept=.8,linetype = 'dotted', col = 'black', size=1)+ xlab("")+
  theme(axis.text.x = element_text(angle = 0))  + 
  theme(legend.position="right")+ylim(0.6,1)+ylab("")+
  theme(panel.grid.major.x = element_line(color = "gray", size = 0.1))+
  guides(size = "none")



```


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# in this case sampling outside the caret resampling procedure

bal_train.7 <- train.7 %>% mutate(Class = as.factor(Class))
bal_test.7  <- test.7 %>% mutate(Class = as.factor(Class))

down_train.7 <- downSample(x = bal_train.7[, -ncol(bal_train.7)], y = bal_train.7$Class)
up_train.7   <- upSample  (x = bal_train.7[, -ncol(bal_train.7)], y = bal_train.7$Class)

table(down_train.7$Class)   
table(up_train.7$Class)   

ctrl <- trainControl(method = "repeatedcv", repeats = 3,#5
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

modelRF7 <- train(Class ~ ., data = bal_train.7, 
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl)
saveRDS(modelRF7, "modelRF7.rds")


modelRF7DN <- train(Class ~ ., data = down_train.7, 
                      method = "rf",
                      metric = "ROC",
                      trControl = ctrl)
saveRDS(modelRF7DN, "modelRF7DN.rds")

modelRF7UP <- train(Class ~ ., data = up_train.7, 
                    method = "rf",
                    metric = "ROC",
                    trControl = ctrl)
 saveRDS(modelRF7UP, "modelRF7UP.rds")


#probabilities that the company belongs to class top
pred.orig.7 <- predict(modelRF7,     newdata=test.7, type = "prob")[,"top"]
pred.up.7   <- predict(modelRF7UP,   newdata=test.7, type = "prob")[,"top"]
pred.down.7 <- predict(modelRF7DN,   newdata=test.7, type = "prob")[,"top"]

library(pROC)
roc.orig.7 <- roc(bal_test.7$Class,  pred.orig.7)
roc.up.7   <- roc(bal_test.7$Class,  pred.up.7)
roc.down.7 <- roc(bal_test.7$Class,  pred.down.7)

aucs.7<- c(orig = roc.orig.7$auc, up=roc.up.7$auc, down=roc.down.7$auc)  %>% bind_rows() %>%
  pivot_longer(everything(),names_to = "method" , values_to = "auc")
```
 

```{r include=FALSE}

ROC7 <- ggplot()+
  geom_line(aes(x=1-roc.orig.7$sensitivities, y=roc.orig.7$specificities, col="orig"))+
  geom_line(aes(x=1-roc.up.7$sensitivities, y=roc.up.7$specificities, col="up"))+
  geom_line(aes(x=1-roc.down.7$sensitivities, y=roc.down.7$specificities, col="down"))+
  geom_line(size = 1, alpha = 1)+theme_minimal()+
  xlab("1 - Specificity") + ylab("Sensitivity") +
   theme(
     plot.title = element_text(size = 10,hjust = 0.5),
     axis.text = element_text(size =10),
     axis.title = element_text(size = 10)) + annotate('segment' , x = 0, xend = 1, y = 0, yend = 1, alpha = 0.7) +
    guides( color = "none")
 #ROC7
```


## 4.4 Performance of decision trees on inbalanced dataset 

In classification problems, a disparity in the frequencies of the observed classes can have a significant negative impact on model fitting. we will first of all assess the performance of decision trees, using the procedure seen above. 

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
tuning.results = tibble() 

for (mspl in seq(1,200,5)){
    tuning.results <- tuning.results %>% 
      bind_rows(compute.Kfold.AccuracySensitivitySpecificity(data8.learn,  k_folds=5, minsplit=mspl, cp=cp_fixed ))
}
```

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
summ.results<-tuning.results %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

 p03 <- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = ac.test, group = f, colour = 'accuracy test')) + 
  geom_line(aes(x=minsplit, y = ac.learn,group = f, colour = 'accuracy learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","red")) +  ylim(0.2,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2],col = 'yellow')

p04<- summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = se.test, group = f, colour = 'sensitivity test')) + 
  geom_line(aes(x=minsplit, y = se.learn,group = f, colour = 'sensitivity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","green"))+  ylim(0.2,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2],col = 'yellow')

p05<-summ.results%>%  ggplot() + theme_minimal()+
  geom_line(aes(x=minsplit, y = sp.test, group = f, colour = 'specificity test')) + 
  geom_line(aes(x=minsplit, y = sp.learn,group = f, colour = 'specificity learn')) + 
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values=c("black","blue"))+  ylim(0.2,1)+ ylab("")+ 
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+
  geom_hline(yintercept = proportions$majority.class[2] ,col = 'yellow')
```


```{r fig.cap=paste("Experimental evaluation of accuracy, sensitivity and selectivity for a single decision tree on the inbalanced dataset y8"), message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
wrap_plots(p03,p04,p05, guides = 'collect')
```


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
optimal.params.8 <- AutoTuneTree(data8.learn, data8.test, 
                               k_folds_testing=3, k_folds_tuning=3, 
                               minsplit_candidates=seq(30,100,1))
```

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}

optimal.minsplit.8  <- optimal.params.8 %>% head(1) %>% select(minsplit) %>% pull
optimal.acc.learn.8 <- optimal.params.8 %>% head(1) %>% select(ac.learn) %>% pull
optimal.acc.test.8  <- optimal.params.8 %>% head(1) %>% select(ac.test) %>% pull

optimal.tree.8 <- rpart(  isTop~.,data8.learn, 
                        method = "class", 
                        minsplit = optimal.minsplit.8, 
                        cp = cp_fixed)
saveRDS(optimal.tree.8, "model_tree_8.rds")

```

```{r echo=FALSE, fig.cap=paste("A visual representation of the binary decision tree optimized fot the imbalanced dataset y8"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
rpart.plot(optimal.tree.8)
```

Even after optimization, a decision tree model has a very poor performance. Accuracy and specificity soar to the level of minority class, while sensitivity drops below 50%. 

```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
summ.results.8<-optimal.params.8 %>% group_by(minsplit)%>%
  summarise(ac.test=mean(ac.test), ac.learn=mean(ac.learn), 
            se.test=mean(se.test), se.learn=mean(se.learn), 
            sp.test=mean(sp.test), sp.learn=mean(sp.learn), f=f, n=n)%>%
  group_by(f) %>%
  mutate(sp.se = sp.test / se.test)

data.to.plot.8 <-  tibble('indic'='', 'method'='', 'value'=0)%>% head(0)#empty tibble 
data.to.plot.8  <- data.to.plot.8 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y8 tree', value =  mean(summ.results.8$ac.test)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y8 tree', value =  mean(summ.results.8$se.test))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y8 tree', value =  mean(summ.results.8$sp.test)))

p11 <- summ.results.8 %>% pivot_longer(cols=c(ac.test, sp.test, se.test)) %>% 
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("summary results for optimal tree - imbalanced dataset")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0,1)+
    geom_hline(yintercept = proportions$majority.class[2],col = 'yellow')

p11


```

This is known as the "accuracy paradox" in unbalanced datasets, where simply predicting the majority class provides high accuracy, associated with a high bias: the model misclassifies the "top" performing companie

## 4.5 Performance of Random Forrest on inbalanced dataset 

Similarly, a Random Forrest model has poor performance. One technique for resolving such a class imbalance is to sub-sample the training data in a manner that mitigates the issues. Examples of sampling methods for this purpose are:

* down-sampling: randomly subset all the classes in the training set so that their class frequencies match the minority class. In the case of y8, about 75% of the observations are the majority class. Down-sampling would randomly sample the majority class to be the same size as the minority class (so that only a part of the training data is used to fit the model). 
* up-sampling: randomly sample (with replacement) the minority class to be the same size as the majority class. caret contains a function (upSample) to do this.
* hybrid methods: down-sample the majority class and synthesize new data points in the minority class. 
The above mentioned methods are implemented in the caret package as "downSample" and "upSample".

The procedure is executed using caret library, specifying sub-sampling when using train so that it is conducted inside of resampling.

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(caret)
set.seed(123)

data8r <- data8 %>% 
  mutate(Class = case_when(isTop== TRUE   ~ "top" , isTop== FALSE   ~ "other")) %>% 
  select(-isTop)  
train.8 <- data8r %>% slice( indexes.learning)
test.8  <- data8r %>% slice(-indexes.learning) 
# prop.table(table(train$Class))
# prop.table(table(test$Class))
# # general control function for training
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,#10
                     repeats = 3,#5
                     summaryFunction = twoClassSummary,
                     savePredictions = "final",
                     classProbs = TRUE)

data.to.plot.rf.8 <- kFoldassesAccuracySensitivityScpecificityRF(data8r, 
                                                                 ctrl,
                                                          name='y8 RF', 
                                                          k_folds=10)
 data.to.plot.8  <- data.to.plot.8 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y8 RF', value =  mean(data.to.plot.rf.8$accuracy)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y8 RF', value =  mean(data.to.plot.rf.8$sensitivity))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y8 RF', value =  mean(data.to.plot.rf.8$specificity)))

ctrl$sampling <- "up" 
data.to.plot.rf.8 <- kFoldassesAccuracySensitivityScpecificityRF(data8r, 
                                                                 ctrl, 
                                                          name='y8 RF UP', 
                                                          k_folds=5) 
 data.to.plot.8  <- data.to.plot.8 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y8 RF UP', value =  mean(data.to.plot.rf.8$accuracy)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y8 RF UP', value =  mean(data.to.plot.rf.8$sensitivity))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y8 RF UP', value =  mean(data.to.plot.rf.8$specificity)))

ctrl$sampling <- "down" 
data.to.plot.rf.8 <- kFoldassesAccuracySensitivityScpecificityRF(data8r, ctrl, 
                                                          name='y8 RF DOWN', 
                                                          k_folds=5)

 data.to.plot.8  <- data.to.plot.8 %>% 
  add_row( tibble_row(indic = 'accuracy',   method =  'y8 RF DOWN', value =  mean(data.to.plot.rf.8$accuracy)))%>%
  add_row( tibble_row(indic = 'sensitivity',method =  'y8 RF DOWN', value =  mean(data.to.plot.rf.8$sensitivity))) %>%
  add_row( tibble_row(indic = 'specificity',method =  'y8 RF DOWN', value =  mean(data.to.plot.rf.8$specificity)))
```


```{r echo=FALSE, fig.height=3, fig.width=8}
p01 <- ggplot(data = data.to.plot.rf.8)+ggtitle("RF (k-fold results)")+ylab("")+xlab("")+
  geom_point( aes(x=f, y=accuracy,color = "accuracy"))+
  geom_point( aes(x=f, y=specificity,color = "specificity"))+
  geom_point( aes(x=f, y=sensitivity,color = "sensitivity"))+ 
  theme(legend.position="top")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0.6,1)+
  scale_x_continuous(breaks = seq(1, 10, by = 1))+
   theme(panel.grid.major.x = element_line(color = "gray",
                                        size = 0.5,
                                        linetype = 1)) 
 
 
 
p02 <- data.to.plot.rf.8 %>% select(-name, -f)%>% 
  pivot_longer(everything(),names_to = "name" , values_to = "value" )  %>%
  ggplot(aes(x=name, y=value, color = name))+
  geom_boxplot(show.legend = FALSE)+ggtitle("RF performance indexes")+ylab("")+xlab("")+
  geom_hline(yintercept = .80, linetype = 'dotted', col = 'black', size=1)+ylim(0.6,1)



wrap_plots(p01,p02,  guides = 'collect')


```


```{r echo=FALSE, fig.cap=paste("Experimantal assessment of accuracy, sensitivity and specificity for a random forrest model on a inbalanced dataset y8, using oversampling, undersampling"), fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}

 data.to.plot.8 <- data.to.plot.8 %>% 
  mutate(method1 = factor(method, levels = c('y8 tree', 'y8 RF', 'y8 RF UP', 'y8 RF DOWN'))) 
data.to.plot.8%>%
  ggplot()+ 
  #geom_line( aes(x = method1, y = value, group = indic, color = indic))+
  geom_point(aes(x = method1, y = value, shape = method, color = indic, size = 4))+
  geom_hline(yintercept=.8,linetype = 'dotted', col = 'black', size=1)+ xlab("")+ylim(0,1)+
  theme(axis.text.x = element_text(angle = 0))  + theme(legend.position="right") +
  theme(panel.grid.major.x = element_line(color = "gray", size = 0.1))+ylab("")+
  guides(size = "none")

```


## 4.6 Comparison of results

The accuracy of best-performing models is summarized in the following table. 
Random Forrest models (RF) ensure consistently higher performance compared to decision tree models. On the balaced dataset the RF model can achieve an accuracy of 0.83, associate with values of sensitivity and specificity, that are higher than the corresponding decision tree (model Tree SENS).
When dealing with the inbalanced data, Random Forrest can achieve results that are in line with the previous case, provided that the learning phase is performed on a suitably oversampled dataset. In this case the oversampling strategy that performed best is "downsampling". 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
bind_rows(data.to.plot.7, data.to.plot.8) %>% 
  select(-method1) %>%
  mutate(dataset = substr(method,start=1,stop=2))%>%
  mutate(method = substr(method, start=3, stop=99))%>%
  group_by(dataset, method)%>%
  pivot_wider(names_from = indic)%>% 
  filter(!method %in% c(" tree",  " RF UP")) %>% 
  mutate(across(where(is.numeric), round, 3))%>%
  knitr::kable()
```


The comparison of performance can be visualized using the ROC Curve (Receiver Operating Characteristics, a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied) measuring the  AUC (Area Under The Curve).


```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# in this case sampling outside the caret resampling procedure

imbal_train.8 <- train.8 %>% mutate(Class = as.factor(Class))
imbal_test.8 <-  test.8 %>% mutate(Class = as.factor(Class))

down_train.8 <- downSample(x = imbal_train.8[, -ncol(imbal_train.8)], y = imbal_train.8$Class)
up_train.8   <- upSample(x = imbal_train.8[, -ncol(imbal_train.8)], y = imbal_train.8$Class)

table(down_train.8$Class) 
table(up_train.8$Class)   

ctrl <- trainControl(method = "repeatedcv", repeats = 3,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

modelRF8 <- train(Class ~ ., data = imbal_train.8, 
                  method = "rf",
                  metric = "ROC",
                  trControl = ctrl)
saveRDS(modelRF8, "modelRF8.rds")


modelRF8DN <- train(Class ~ ., data = down_train.8, 
                      method = "rf",
                      metric = "ROC",
                      trControl = ctrl)
saveRDS(modelRF8, "modelRF8dn.rds")

modelRF8UP <- train(Class ~ ., data = up_train.8, 
                    method = "rf",
                    metric = "ROC",
                    trControl = ctrl)
saveRDS(modelRF8, "modelRF8up.rds")
 

#probabilities that the company belongs to class top
pred.orig.8 <- predict(modelRF8,     newdata=test.8, type = "prob")[,"top"]
pred.up.8   <- predict(modelRF8UP,   newdata=test.8, type = "prob")[,"top"]
pred.down.8 <- predict(modelRF8DN,   newdata=test.8, type = "prob")[,"top"]
 
```

```{r error=FALSE, message=FALSE, warning=FALSE, include=FALSE}

library(pROC)
roc.orig.8 <- roc(imbal_test.8$Class,  pred.orig.8)
roc.up.8   <- roc(imbal_test.8$Class,  pred.up.8)
roc.down.8 <- roc(imbal_test.8$Class,  pred.down.8)

aucs.8<- c(orig = roc.orig.8$auc, up=roc.up.8$auc, down=roc.down.8$auc)  %>% bind_rows() %>%
  pivot_longer(everything(),names_to = "method" , values_to = "auc")

```


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
ROC8 <- ggplot()+
  geom_line(aes(x=1-roc.orig.8$sensitivities, y=roc.orig.8$specificities, col="orig"))+
  geom_line(aes(x=1-roc.up.8$sensitivities, y=roc.up.8$specificities, col="up"))+
  geom_line(aes(x=1-roc.down.8$sensitivities, y=roc.down.8$specificities, col="down"))+
  geom_line(size = 1, alpha = 1)+theme_minimal()+
  xlab("1 - Specificity") + ylab("Sensitivity") +
   theme(
     plot.title = element_text(size = 10,hjust = 0.5),
     axis.text = element_text(size =10),
     axis.title = element_text(size =10)) + annotate('segment' , x = 0, xend = 1, y = 0, yend = 1, alpha = 0.7) +
      guides(size = "none", color = "none")+
    guides( color = "none")

```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
aucs.7<-aucs.7%>%mutate(bal = "balanced")
aucs.8<-aucs.8%>%mutate(bal = "inbalanced")
table.auc <- bind_rows(aucs.7,aucs.8) %>%
  filter( (method == 'orig' & bal == 'balanced') | 
            # (method == 'orig' & bal == 'inbalanced') |
            (method == 'down' & bal == 'inbalanced')) %>%
  mutate(across(where(is.numeric), round, 3))%>%
  mutate(dataset = bal)%>% 
  select(-bal)%>%
  select(dataset, method, auc)
knitr::kable(table.auc)
```


```{r include=FALSE}
compare.auc<-bind_rows(aucs.7,aucs.8)
compare.auc.plot <- compare.auc%>% 
  ggplot(aes(x=method, y=auc, group=bal))+ylim(0.85,.95)+
    geom_line(aes(color = bal))+
    geom_point(aes(shape=bal, color = bal, size = 6))+
    theme(panel.grid.major.x = element_line(color = "gray", size = 0.1))+
    theme(axis.text.x = element_text(angle = 0), axis.title.x = element_text(angle = 0))+ 
    theme(axis.text.y = element_text(angle = 0), axis.title.y = element_text(angle = 90))+ 
    theme(legend.position="right") +
    guides(shape = guide_legend(title = "dataset"))+
    guides(size="none")+    guides( bal="none")+
    xlab("oversampling methods") + ylab("AUC") +
    labs(title="AUC - Area Under the Curve")+ ylim(0.5,1)
```



```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

ROC.comp <- ggplot()+
  #geom_line(aes(x=1-roc.orig.8$sensitivities, y=roc.orig.8$specificities, col="imbalanced"))+
  geom_line(aes(x=1-roc.down.8$sensitivities, y=roc.down.8$specificities, col="imbalanced-down"))+
  geom_line(aes(x=1-roc.orig.7$sensitivities, y=roc.orig.7$specificities, col="balanced"))+
  theme_minimal()+
  xlab("FPR = 1 - Specificity") + ylab("TPR = Sensitivity") +
   theme(
     plot.title = element_text(size = 10,hjust = 0.5),
     axis.text = element_text(size =10),
     axis.title = element_text(size =10)) + 
     annotate('segment' , x = 0, xend = 1, y = 0, yend = 1, alpha = 0.7, colour="gray") +
     scale_colour_manual("", 
          breaks = c("imbalanced",  "imbalanced-down", "balanced"),
          values = c("blue", "green", "red")) +
     labs(title="Random Forrest: comparison of ROC curves")
  
```

```{r echo=FALSE, fig.height=3, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}

ROC.comp

```
The AUC index is similar for both curves, hence the predictions on an inbalanced dataset can be expected to achieve the same quality of predictions on a balanced dataset.


# 5. Concluding remarks  

This project demonstrates that a machine learning algorithm can predict whether a company belongs to the group of "top performers" with accuracy, sensitivity of the order of 80%, and that the same quality of predictions can be achieved also for strongly inbalanced datasets. 

The dataset used for learning and validation is composed of n = `r nrow(data)` observations and p = `r ncol(data)` features, obtained from a larger dataset of regional companies, filtered on NACE sectors of activity and on year 2019. 
The performance has been assessed experimentally for a binary classification algorithm (namely a single tree, generated by the recursive partitioning algorithm of r-part library) and for a random Forrest.
Two case studies have been examined in detail, using different definitions of the y label (isTop8, a narrow definition addressing about 15% of the sample) and isTop7 (a balanced dataset addressing about half of the companies).

The model has been tuned to maximize accuracy on test data (checking that sensitivity and specificity are within an acceptable range) using a nested k-fold cross validation procedure. The imbalanced datasets performs equally well with a *random Forrest* model, earned using an oversampling of the minority class, to achieve a good accuracy, and improving on sensitivity and specificity.

In both cases computation time is of the order of minutes, depending on the number of folds in k-fold cross validation, which is in line with the constraints of the project. The accuracy of predictions has been measured experimentally, and results are in all cases around 80%. Specifically, key results are summarized in the following table.



The quality of predictions depends on the selection of companies included in the dataset: we may expect that smaller, homogeneous sets may result in higher accuracy (e.g. selecting only companies of a given size, or of a single NACE sector) and the exact definition of the label. In future applications the model *should be learned for specific subsets and specific definitions of label*, and accuracy (or quality of predictions) has to be evaluated for each case. 
 

These results suggest further investigations. First, as the optimal decision trees are simple and do not use many of the available features, we should assess the predictive power of each feature and work on a smaller model with the advantage of improving interpretability and explainability. Second, explore other supervised ML methods such as  multi-class decision trees, SVM, naive Bayes and KNN. Moreover, some unsupervised techniques may be applied, such as Principal Component Analysis, estimate of intrinsic dimension and k-means clustering. 

Finally, as data is available also for 2018 and 2020, we may compare the performance of models on same-year data as well as using unseen data from other years. A relevant question is to train a model on data from 2018 and 2019, and compare predictions on 2020, when  disruptive change in economy took place.

# References




